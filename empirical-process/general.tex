\chapter{}

\section{Concentration by Entropic Techniques}

\begin{definition}[Entropy]
	The entropy of a random variable \(X\) for the convex function \(\phi(\cdot)\) is defined as
	\begin{equation}
		H_{\phi}(X)=\bbE[\phi(X)]-\phi(\bbE[X])
	\end{equation}
\end{definition}

\begin{example}
	\begin{enumerate}
		\item For \(\phi(u)=u^2\), \(H_{\phi}(X)=\Var(X)\)
		\item For \(\phi(u)=-\log(u)\) (\(u>0\)), and for \(X\) real-valued random variable, we have that \(Z:=\exp(\lambda X)\) is a non-negative random variable, and
		      \begin{equation}
			      H_{\phi}(Z)=-\lambda\bbE[X]+\log(\bbE[\exp(\lambda X)])=\log\bbE\mathrm{e}^{\lambda(X-\bbE[X])}.
		      \end{equation}
	\end{enumerate}
\end{example}

\begin{definition}
	Let \(\Omega\) be a finate sample space and denote \(\mcM(\Omega)\) as the set of all probability measures (vectors) on \(\Omega\).
	\begin{enumerate}
		\item The \textbf{relative entropy} with respect to \(q\in\mcM(\Omega)\) is defined as the mapping \(H(\cdot\mid q):\mcM(\Omega)\to[0, \infty]:p\mapsto H(p\mid q)\), where
		      \begin{equation}
			      H(p\mid q)=\left\{
			      \begin{array}{ll}
				      \sum_{\omega\in\Omega}p(\omega)\log\left(\frac{p(\omega)}{q(\omega)}\right) & \text{if }p\ll q \\
				      +\infty                                                                     & \text{otherwise}
			      \end{array}
			      \right.
		      \end{equation}
		\item The \textbf{Shannon entropy} of a \(\Omega\)-valued random variable \(X\) with distribution \(p\in\mcM(\Omega)\) is defined as
		      \begin{equation}
			      \mcH(p)=-\sum_{\omega\in\Omega}p(\omega)\log(p(\omega)).
		      \end{equation}
	\end{enumerate}
\end{definition}

\begin{proposition}[Duality formula of the Entropy]

\end{proposition}

\begin{lemma}
	Let \(\Omega\) be a finite sample space and denote \(\mcM(\Omega)\) as the set of probability measures on \(\Omega\). Let \(q\in\mcM(\Omega)\), then the relative entropy \(H(\cdot\mid q)\) is strictly convex, continuous and
	\begin{equation}
		H(p\mid q)=0\iff p=q.
	\end{equation}
\end{lemma}

\begin{proof}
	To prove this theorem, we analyze the properties of the relative entropy (also known as the Kullback-Leibler divergence). Let \(\Omega\) be a finite sample space, \(\mathcal{M}(\Omega)\) denote the set of probability measures on \(\Omega\), and let \(q \in \mathcal{M}(\Omega)\) be a fixed probability measure. For any \(p \in \mathcal{M}(\Omega)\), the relative entropy \(H(p \mid q)\) is defined as
	\[
		H(p \mid q) = \sum_{\omega \in \Omega} p(\omega) \log \frac{p(\omega)}{q(\omega)},
	\]
	where we assume \(q(\omega) > 0\) for all \(\omega \in \Omega\) (otherwise, the term is taken as \(0\)).

	\paragraph*{Non-negativity of Relative Entropy}
	First, observe that for any \(p \in \mathcal{M}(\Omega)\), by Jensen's inequality, we have
	\[
		\sum_{\omega \in \Omega} p(\omega) \log \frac{p(\omega)}{q(\omega)} \geq 0,
	\]
	with equality if and only if \(p = q\). Thus, we obtain \(H(p \mid q) \geq 0\), and \(H(p \mid q) = 0 \iff p = q\). This proves
	\[
		H(p \mid q) = 0 \iff p = q.
	\]

	\paragraph*{Strict Convexity of Relative Entropy}
	To prove that \(H(\cdot \mid q)\) is strictly convex, we consider the dependence of the relative entropy \(H(p \mid q)\) on \(p\). Let \(p_1, p_2 \in \mathcal{M}(\Omega)\) with \(p_1 \neq p_2\). For \(0 < \lambda < 1\), define
	\[
		p_\lambda = \lambda p_1 + (1 - \lambda) p_2.
	\]
	Using the definition of relative entropy, we have
	\[
		H(p_\lambda \mid q) = \sum_{\omega \in \Omega} p_\lambda(\omega) \log \frac{p_\lambda(\omega)}{q(\omega)}.
	\]
	Applying Jensen's inequality for strictly convex functions, we obtain
	\[
		H(p_\lambda \mid q) < \lambda H(p_1 \mid q) + (1 - \lambda) H(p_2 \mid q).
	\]
	This shows that \(H(\cdot \mid q)\) is strictly convex.

	\paragraph*{Continuity of Relative Entropy}
	Finally, we prove that the relative entropy \(H(p \mid q)\) is continuous with respect to \(p\). Since \(\Omega\) is finite, in this finite-dimensional vector space, \(p \mapsto H(p \mid q)\) is a sum of a finite number of terms, each of which is a continuous function of \(p(\omega) \log(p(\omega)/q(\omega))\). Thus, \(H(p \mid q)\) is continuous with respect to \(p\).

\end{proof}

\begin{equation}
	H(\exp(\lambda X))=\lambda M'_{X}(\lambda)-M_{X}(\lambda)\log M_{X}(\lambda).
\end{equation}

\begin{proposition}[Herbst argument]
	Let \(X\) be a random variable and suppose that for \(\sigma>0\),
	\begin{equation}
		\mcH(\mathrm{e}^{\lambda X})\leq\frac{\lambda^2\sigma^2}{2}M_{X}(\lambda),
	\end{equation}
	for \(\lambda\in I\) with interval \(I\) being either \(\bbR\) or \([0, \infty)\). Then,
	\begin{equation}
		\log\bbE\mathrm{e}^{\lambda(X-\bbE[X])}\leq\frac{\lambda^2\sigma^2}{2},\quad\forall\lambda\in I.
	\end{equation}
\end{proposition}

\begin{proposition}[Bernstein entropy bound]
	Suppose there exists \(B>0\) and \(\sigma>0\) such that

\end{proposition}

\begin{definition}[Separately convex]
	A function \(f:\bbR^{n}\to\bbR\) is said to be \textbf{separately convex} if for all \(k\in[n]\), the function \(f(\cdot, x_{-k})\) is convex for all \(x_{-k}\in\bbR^{n-1}\).
\end{definition}

\begin{definition}[Lipschitz continuous]
\end{definition}

\begin{definition}[Globally Lipschitz continuous]
\end{definition}
\begin{lemma}[Entropy bound for univariate functions]\label{lem:entropy-bound-univariate}
	Let \(X\) and \(Y\) two independent, identically distributed \(\mathbb{R}\)-valued random variables. Denote by \(\bbE_{X, Y}\) the expectation with respect to \(X\) and \(Y\). For any function \(g: \mathbb{R} \rightarrow \mathbb{R}\) the following statements hold:
	\begin{enumerate}
		\item \(\forall\lambda>0\), \(\mcH\left(\mathrm{e}^{\lambda g(X)}\right) \leq \lambda^2 \bbE_{X, Y}\left[(g(X)-g(Y))^2\mathrm{e}^{\lambda g(X)}\bfI\{g(X) \geq g(Y)\}\right]\).
		\item If in addition the random variable \(X\) is supported on \([a, b]\), \(a<b\), and the function \(g\) is convex and Lipschitz continuous, then
		      \begin{equation*}
			      \mcH\left(\mathrm{e}^{\lambda g(X)}\right) \leq \lambda^2(b-a)^2 \bbE\left[\left(g^{\prime}(X)\right)^2 \mathrm{e}^{\lambda g(X)}\right],\quad\forall\lambda>0.
		      \end{equation*}
	\end{enumerate}
\end{lemma}

\begin{proof}
	Using the fact that \(X\) and \(Y\) are independent and identically distributed, we have
	\begin{equation*}
		\mcH\left(\mathrm{e}^{\lambda g(X)}\right)=\bbE_{X}\left[\lambda g(X)\mathrm{e}^{\lambda g(X)}\right]-\bbE_{X}\left[\lambda g(X)\right]\log\bbE_{Y}\mathrm{e}^{\lambda g(Y)}.
	\end{equation*}
\end{proof}

\begin{lemma}[Tensorisation of the entropy]\label{lem:tensorisation-entropy}
	Let \(X_{1}, \ldots, X_{n}\) be independent real-valued random variables and \(f:\bbR^{n}\to\bbR\) be a given function. Then, for all \(\lambda>0\),
	\begin{equation*}
		\mcH(\mathrm{e}^{\lambda f(X_{1}, \ldots, X_{n})})\leq\sum_{i=1}^{n}\mcH(\mathrm{e}^{\lambda f_{i}(X_{i})}\mid \bfX_{-i}),
	\end{equation*}
	where \(f_{i}:\bbR\to\bbR\) is defined as \(f_{i}(x)=f(x_{1}, \ldots, x_{i-1}, x, x_{i+1}, \ldots, x_{n})\).
\end{lemma}

\begin{proof}
	According to the variational representation of the entropy, we have
	\begin{equation*}
		\mcH(\mathrm{e}^{\lambda f(\bfX)})=\sup_{g\in\mcG}\left\{\bbE[g(\bfX)\mathrm{e}^{\lambda f(\bfX)}]\right\},
	\end{equation*}
	where \(\mcG=\left\{g:\Omega\to\bbR:\mathrm{e}^{g}\leq 1\right\}\).

	For each \(i\in[n]\), define \(\bfX_{i}=(X_{i},\ldots, X_{n})\) and for any \(g\in\mcG\) define \(g^{i}\), \(i\in[n]\):
	\begin{equation*}
		\begin{aligned}
			g^{1}(\bfX)=     & g(\bfX)-\log\bbE\left[\mathrm{e}^{g(\bfX)}\mid \bfX_{2}\right],                                                                      \\
			g^{i}(\bfX_{i})= & \log\frac{\bbE\left[\mathrm{e}^{g(\bfX)}\mid \bfX_{i}\right]}{\bbE\left[\mathrm{e}^{g(\bfX)}\mid \bfX_{i+1}\right]},\quad i\in[n-1].
		\end{aligned}
	\end{equation*}

	It is easy to see that by the above construction, we have
	\begin{equation}
		\label{eq:tensorisation-entropy-1}
		\sum_{i=1}^{n}g^{i}(\bfX)=g(\bfX)-\log\bbE\left[\mathrm{e}^{g(\bfX)}\right]\geq g(\bfX),
	\end{equation}
	and
	\begin{equation*}
		\bbE\left[\exp\left(g^{i}(\bfX_{i})\mid X_{i+1}\right)\right]=1.
	\end{equation*}

	Within the variational representation of the entropy, we have
	\begin{equation*}
		\begin{aligned}
			\bbE\left[g(\bfX)\mathrm{e}^{\lambda f(\bfX)}\right] & \underbrace{\leq}_{\eqref{eq:tensorisation-entropy-1}}\bbE\left[\sum_{i=1}^{n}g^{i}(\bfX)\mathrm{e}^{\lambda f(\bfX)}\right] \\
			                                                     & =\sum_{i=1}^{n}\bbE\left[g^{i}(\bfX)\mathrm{e}^{\lambda f(\bfX)}\right]                                                      \\
			                                                     & \leq
		\end{aligned}
	\end{equation*}
\end{proof}

\begin{theorem}[Tail-bound for Lipschitz functions]\label{thm:tail-bound-lipschitz}
	Let \(\bfX\in\bbR^{n}\) be a random vector with independent coordinates \(X_{i}\) supporrted on the interval \([a,b], a<b\) and let \(f:\bbR^{n}\to\bbR\) be separately convex and L-Lipschitz continuous with respect to the Euclidean norm. Then, for all \(t>0\),
	\begin{equation}
		\bbP\left(f(\bfX)-\bbE[f(\bfX)]\geq t\right)\leq\exp\left(-\frac{t^2}{4L^2(b-a)^2}\right).
	\end{equation}
\end{theorem}

\begin{proof}
	For \(i\in[n]\), and every \(\bfx_{-i}\in\bbR^{n-1}\), the function \(f_{i}(x)\) is convex, and thus by Lemma~\ref{lem:entropy-bound-univariate}, for all \(\lambda>0\) that for every fixed \(\bfx_{-i}\), we have
	\begin{equation*}
		\mcH\left(\mathrm{e}^{\lambda f_{i}(X_{i})}\mid\bfX_{-i}\right)\leq\lambda^2(b-a)^2\bbE\left[\left(f_{i}^{\prime}(X_{i})\right)^2\mathrm{e}^{\lambda f_{i}(X_{i})}\mid\bfX_{-i}\right].
	\end{equation*}

\end{proof}

\begin{example}[Operator norm of a random matrix]
	Let \(M\in\bbR^{n\times d}\) be a random matrix with independent identically distributed mean-zero random entries \(M_{ij}\) supported on the interval \([-1,1]\).

	% \paragraph{Separately convex and Lipschitz continuous} The operator norm \(M\mapsto\|M\|\) is a function \(f:\bbR^{n\times d}\mapsto\bbR\) that is separately convex and 1-Lipschitz continuous with respect to the Euclidean norm.

	\begin{equation}
		\|M\|=\max_{v\in\bbS^{d-1}}\|Mv\|_{2}=\max_{u\in\bbS^{n-1}, v\in\bbS^{d-1}}\langle u, Mv\rangle.
	\end{equation}
	The operator norm is maximin/supermum of functions that are linear in the entries of \(M\), and thus any such function is convex and as such separately convex.
	Moreover, for any \(M, M'\in\bbR^{n\times d}\), we have
	\begin{equation*}
		\begin{aligned}
			\left|\|M\|-\|M'\|\right| & =\left|\max_{u\in\bbS^{n-1}, v\in\bbS^{d-1}}\langle u, Mv\rangle-\max_{u\in\bbS^{n-1}, v\in\bbS^{d-1}}\langle u, M'v\rangle\right| \\
			                          & \leq\max_{u\in\bbS^{n-1}, v\in\bbS^{d-1}}\left|\langle u, Mv\rangle-\langle u, M'v\rangle\right|                                   \\
			                          & \leq\max_{u\in\bbS^{n-1}, v\in\bbS^{d-1}}\|u\|_{2}\|v\|_{2}\|M-M'\|_{F}                                                            \\
			                          & \leq\|M-M'\|_{F},
		\end{aligned}
	\end{equation*}
	where \(\|\cdot\|_{F}\) denotes the Frobenius norm, which is equivalent to the Euclidean norm for matrices.
	Thus, the operator norm is 1-Lipschitz continuous with respect to the Euclidean norm.
	Then by Theorem~\ref{thm:tail-bound-lipschitz}, we have
	\begin{equation}
		\bbP\left(\|M\|-\bbE[\|M\|]\geq t\right)\leq\exp\left(-\frac{t^2}{16}\right).
	\end{equation}
\end{example}

\begin{proof}

\end{proof}

\section{Some Matrix Calculus and Covariance Estimation}

\begin{theorem}[Matrix Bernstein Inequality]\label{thm:matrix-bernstein}
	Let \(X_{1}, \ldots, X_{n}\) be independent, mean-zero random systematic matrices in \(\bbR^{d\times d}\) such that \(\|X_{i}\|\leq K\) almost surely for all \(i\in[n]\). Then for all \(t>0\), we have
	\begin{equation*}
		\bbP\left(\left\|\sum_{i=1}^{n}X_{i}\right\|\geq t\right)\leq 2d\exp\left(-\frac{t^2}{\sigma^{2}+Kt/3}\right),
	\end{equation*}
	where \(\sigma^{2}=\left\|\sum_{i=1}^{n}\bbE[X_{i}^2]\right\|\) is the norm of the matrix variance of the sum.
\end{theorem}

\begin{proof}
	Denote \(S_{n}=\sum_{i=1}^{n}X_{i}\) and \(\lambda_{\max}(S_{n})\) as the largest eigenvalue of \(S_{n}\). Then, we have
	\begin{equation*}
		\|S_{n}\|=\max\{\lambda_{\max}(S_{n}), -\lambda_{\min}(S_{n})\}.
	\end{equation*}

	Since
	\begin{equation*}
		\bbP\left(\lambda_{\max}(S_{n})\geq t\right)=\bbP\left(\exp(\lambda\lambda_{\max}(S_{n}))\geq\exp(\lambda t)\right)\leq\frac{\bbE[\exp(\lambda\lambda_{\max}(S_{n}))]}{\exp(\lambda t)}.
	\end{equation*}
\end{proof}

\begin{lemma}[Bound on MGF]
	Let \(X\) be an \(d\times d\) symmetric mean-zero random matrix such that \(\|X\|\leq K\) almost surely. Then, for \(|\lambda|<3/K\), we have
	\begin{equation*}
		\bbE[\exp(\lambda X)]\preceq\exp\left(g(\lambda)\bbE[X^2]\right),
	\end{equation*}
	where \(g(\lambda)=\frac{\lambda^2/2}{1-|\lambda|K/3}\).
\end{lemma}

\begin{proof}

\end{proof}

\begin{proposition}[Expectation Bound via the Bernstein Inequality]
	Under the conditions of Theorem~\ref{thm:matrix-bernstein}, we have the tail bound
	\begin{equation*}
		\bbP\left(\left\|\sum_{i=1}^{n}X_{i}\right\|\geq t\right)\leq 2d\exp\left(-\frac{t^2}{\sigma^{2}+Kt/3}\right).
	\end{equation*}
	Then,
	\begin{equation*}
		\bbE\left[\left\|\sum_{i=1}^{n}X_{i}\right\|\right]\leq
	\end{equation*}

\end{proposition}

\chapter{Basic Tools in High-dimensional Probability}

\section{Decoupling}

\begin{definition}[Chaos]
	Let \(X_{1},\ldots, X_{n}\) be independent real-valued random variables, and \(a_{ij}\in\bbR\), \(i, j\in[n]\). The random quadratic form
	\begin{equation*}
		\sum_{i,j=1}^{n}a_{ij}X_{i}X_{j}=X^{\top}AX,\quad X=(X_{1},\ldots, X_{n})^{\top}\in\bbR^{n},\quad A=(a_{ij})\in\bbR^{n\times n},
	\end{equation*}
	is called \textbf{chaos} in probability theory.
\end{definition}

For simplicity, we assume that the random variables \(X_{i}\)  have mean zero and unit variance, i.e., \(\bbE[X_{i}]=0\) and \(\bbE[X_{i}^2]=1\), for all \(i\in[n]\). Then,
\begin{equation*}
	\bbE\left[X^{\top}AX\right]=\sum_{i,j=1}^{n}a_{ij}\bbE[X_{i}X_{j}]=\operatorname{Tr}(A).
\end{equation*}

We shall study concentration properties for chaos. This time we need to develop tools to overcome the fact that we have sums of not necessarily independent random variables. The idea is to use the decoupling technique. The idea is to study the following random quadratic form,
\begin{equation*}
	\sum_{i,j=1}^{n}a_{ij}X_{i}X_{j}'=X^{\top}AX'=\langle X, AX'\rangle,
\end{equation*}
where \(X'=(X_{1}',\ldots, X_{n}')^{\top}\) is an independent copy of \(X\), and condition on \(X\). Obvious, the bilinear form is easier to handle, e.g., when we condition on \(X'\), we simply obtain a linear form in \(X\), and vice versa, i.e.,
\begin{equation*}
	\langle X, AX'\rangle=\sum_{i=1}^{n}c_{i}X_{i},\quad c_{i}=\sum_{j=1}^{n}a_{ij}X_{j}',
\end{equation*}
is a random linear form in \(X\) depending on the condition of the independent copy \(X'\).

\begin{theorem}[Decoupling]
	Let \(A\) be an \(n\times n\) diagonal free matrix, \(X=(X_{1},\ldots, X_{n})\in\bbR^{n}\) be a random vector with independent mean-zero coordinates \(X_{i}\), and \(X'\) be an independent copy of \(X\). Then, for every convex function \(f:\bbR\to\bbR\), we have
	\begin{equation*}
		\bbE[f(\langle X, AX\rangle)]\leq\bbE[f(4\langle X, AX'\rangle)].
	\end{equation*}
\end{theorem}

\begin{proof}
	The idea is to study the partial chaos
	\begin{equation*}
		\langle X, AX\rangle=\sum_{(i,j)\in I\times I^{\complement}}a_{ij}X_{i}X_{j},
	\end{equation*}
	with a random subset \(I\subset[n]\). Let \(\delta_{i}\) be independent Bernoulli random variables with \(\bbP[\delta_{i}=0]=\bbP[\delta_{i}=1]=1/2\), and define the random subset \(I=\{i\in[n]:\delta_{i}=1\}\). Then, we condition on \(X\), and
\end{proof}

\begin{theorem}[Hanson-Wright Inequality]
	Let \(X=(X_{1},\ldots, X_{n})\in\bbR^{n}\) be a random vector with independent mean-zero sub-Gaussian coordinates \(X_{i}\) and let \(A\in\bbR^{n\times n}\) be a deterministic matrix. Then, for all \(t\geq0\), we have
	\begin{equation*}
		\bbP\left(\left|\langle X, AX\rangle-\bbE[\langle X, AX\rangle]\right|\geq t\right)\leq 2\exp\left(-c\min\left(\frac{t^2}{K^{4}\|A\|_{F}^{2}}, \frac{t}{K^{2}\|A\|}\right)\right),
	\end{equation*}
	where \(K:=\max_{i\in[n]}\|X_{i}\|_{\psi_{2}}\), and \(c>0\) is a constant.
\end{theorem}

We prepare the proof of the Hanson-Wright inequality by proving the following lemmas.

\begin{lemma}[MGF of Gaussian Chaos]
	Let \(X,X'\sim\mcN(0, I_{n})\), \(X\) and \(X'\) be independent, and let \(A\in\bbR^{n\times n}\) be a deterministic matrix. Then,
	\begin{equation*}
		\bbE[\exp(\lambda\langle X, AX'\rangle)]\leq\exp\left(C\lambda^{2}\|A\|_{F}^{2}\right),\quad\forall|\lambda|\leq\frac{C}{\|A\|},
	\end{equation*}
	where \(C>0\) is a constant.
\end{lemma}

\begin{proof}
	We use the singular value decomposition of the matrix \(A\), i.e.,
	\begin{equation*}
		A=\sum_{i=1}^{n}\sigma_{i}u_{i}v_{i}^{\top},
	\end{equation*}
	then,
	\begin{equation*}
		\langle X, AX'\rangle=\sum_{i=1}^{n}\sigma_{i}\langle X, u_{i}\rangle\langle X', v_{i}\rangle=\sum_{i=1}^{n}\sigma_{i}Y_{i}Y_{i}',
	\end{equation*}
	where \(Y=(\langle X, u_{1}\rangle,\ldots, \langle X, u_{n}\rangle)\sim\mcN(0, I_{n})\) and \(Y'=(\langle X', v_{1}\rangle,\ldots, \langle X', v_{n}\rangle)\sim\mcN(0, I_{n})\) are independent Gaussian vectors. Then independence of the Gaussian vectors \(Y\) and \(Y'\) implies that
	\begin{equation*}
		\bbE[\exp(\lambda\langle X, AX'\rangle)]=\bbE[\exp(\lambda\sum_{i=1}^{n}\sigma_{i}Y_{i}Y_{i}')]=\prod_{i=1}^{n}\bbE[\exp(\lambda\sigma_{i}Y_{i}Y_{i}')].
	\end{equation*}
	For each \(i\in[n]\), we compute the expectation with respect to \(Y'\), i.e., the conditional expectation holding the random vector \(Y\) fixed,
	\begin{equation*}
		\bbE_{Y}\left[\bbE_{Y'}\left[\exp(\lambda\sigma_{i}Y_{i}Y_{i}')\mid Y\right]\right]=\bbE_{Y}\left[\exp\left(\frac{\lambda^{2}\sigma_{i}^{2}Y_{i}^{2}}{2}\right)\right]=\exp\left(\frac{C\lambda^{2}\sigma_{i}^{2}}{2}\right),\quad \forall\lambda\leq\frac{C}{\sigma_{i}},
	\end{equation*}
	where the first equality follows from the fact that the MGF of a Gaussian random variable is given by \(\bbE[\exp(\lambda Y)]=\exp(\lambda^{2}/2)\), and the second equality follows from the fact that the random variable \(Y_{i}\) is Gaussian and thus sub-Gaussian, and therefore \(Y_{i}^{2}\) is sub-exponential, and thus gives the bound. Then, we obtain
	\begin{equation*}
		\bbE[\exp(\lambda\langle X, AX'\rangle)]=\prod_{i=1}^{n}\exp\left(\frac{C\lambda^{2}\sigma_{i}^{2}}{2}\right)=\exp\left(C\lambda^{2}\sum_{i=1}^{n}\sigma_{i}^{2}\right),\quad\forall\lambda\leq\frac{C}{\max_{i\in[n]}\sigma_{i}}.
	\end{equation*}
	Finally, we use the fact that the Frobenius norm of the matrix \(A\) is given by \(\|A\|_{F}^{2}=\sum_{i=1}^{n}\sigma_{i}^{2}\), and the operator norm of the matrix \(A\) is given by \(\|A\|=\max_{i\in[n]}\sigma_{i}\), and thus we obtain the desired bound.
\end{proof}

\begin{lemma}[Comparison]
	Let \(X\) and \(X'\) be independent mean-zero sub-Gaussian random vectors in \(\bbR^{n}\) with \(\|X\|_{\psi_{2}}\leq K\) and \(\|X'\|_{\psi_{2}}\leq K\). Furthermore, let \(Y\) and \(Y'\) be independent mean-zero Gaussian random vectors in \(\bbR^{n}\) with \(Y,Y'\sim\mcN(0, I_{n})\), and let \(A\in\bbR^{n\times n}\) be a deterministic matrix. Then, for all \(\lambda\in\bbR\), we have
	\begin{equation*}
		\bbE[\exp(\lambda\langle X, AX\rangle)]\leq\bbE[\exp(CK^{2}\lambda\langle Y, AY\rangle)].
	\end{equation*}
\end{lemma}

\begin{proof}
	We condtion on \(X'\) and take the expectation with respect to \(X\), then, \(\langle X, AX'\rangle\) is conditionally sub-Gaussian and we have
	\begin{equation*}
		\bbE_{X}[\exp(\lambda\langle X, AX'\rangle)]\leq\bbE[\exp(CK^{2}\lambda\|AX'\|_{2}^{2})],\quad\forall\lambda\in\bbR.
	\end{equation*}
	We now replace \(X\) by \(Y\) but still condition on \(X'\), then we have
	\begin{equation*}
		\bbE_{Y}[\exp(\mu\langle Y, AX'\rangle)]\leq\bbE[\exp(\mu^2\|AX'\|_{2}^{2}/2)],\quad\forall\mu\in\bbR.
	\end{equation*}
	Choosing \(\mu=\sqrt{2C\lambda}K\), we can match our estimates to get
	\begin{equation*}
		\bbE_{X}[\exp(\lambda\langle X, AX'\rangle)]\leq\bbE_{Y}[\exp(CK^{2}\lambda\langle Y, AX'\rangle)]=\exp(CK^{2}\lambda\|AX'\|_{2}^{2}).
	\end{equation*}
	We can now take the expectation with respect to \(X'\) on both sides, repeate the same procedure for the \(X'\) and \(Y'\) to obtain the desired bound.
\end{proof}

\begin{proof}
	Wihtout loss of generality, \(K=1\). It suffices to show the one sided bound. Denote
	\begin{equation*}
		p=\bbP\left(\langle X, AX\rangle-\bbE[\langle X, AX\rangle]\geq t\right).
	\end{equation*}
	We can write
	\begin{equation*}
		\langle X, AX\rangle=\sum_{i=1}^{n}a_{ii}X_{i}^{2}+\sum_{i\neq j}a_{ij}X_{i}X_{j},
	\end{equation*}
	and thus the problem reduces to bounding the tail of the diagonal sums and the off-diagonal sums separately:
	\begin{equation*}
		p\leq\bbP\left(\sum_{i=1}^{n}a_{ii}\left(X_{i}^{2}-\bbE\left[X_{i}^{2}\right]\right)\geq t/2\right)+\bbP\left(\sum_{i\neq j}a_{ij}\left(X_{i}X_{j}\right)\geq t/2\right):=p_{1}+p_{2}.
	\end{equation*}
	We shall bound the two terms separately. For the diagonal terms, since \(X_i^2-\bbE[X_i^2]\) are independent mean-zero sub-Eponential random variables, we have
	\begin{equation*}
		\|X_{i}^{2}-\bbE[X_{i}^{2}]\|_{\psi_{1}}\leq C\|X_{i}^{2}\|_{\psi_{1}}\leq C\|X_{i}\|_{\psi_{2}}^{2}\leq C,
	\end{equation*}
	and thus by the Bernstein inequality, we have
	\begin{equation*}
		p_{1}\leq\exp\left(-C\min\left(\frac{t^{2}}{\|A\|_{F}^{2}}, \frac{t}{\|A\|}\right)\right).
	\end{equation*}
	For the off-diagonal terms, denote \(S:=\sum_{i\neq j}a_{ij}X_{i}X_{j}\), then we have
\end{proof}

\section{Concentration}
% for Anisotropic Random Vectors

\begin{lemma}
	Let \(B\in\bbR^{m\times n}\) be a deterministic matrix, and let \(X\in\bbR^{n}\) be an isotropic random vector \(\in\bbR^{n}\), then
	\begin{equation*}
		\bbE\left[\|BX\|_{2}^{2}\right]=\|B\|_{F}^{2}.
	\end{equation*}
\end{lemma}

\begin{proof}
	We have
	\begin{equation*}
		\bbE\left[\|BX\|_{2}^{2}\right]=\bbE\left[\sum_{i=1}^{m}\left(\sum_{j=1}^{n}b_{ij}X_{j}\right)^{2}\right]=\sum_{i=1}^{m}\sum_{j=1}^{n}b_{ij}^{2}\bbE[X_{j}^{2}]=\|B\|_{F}^{2}.
	\end{equation*}
\end{proof}

\section{Symmetrisation}

\begin{definition}[Symmetric Random Variables]
	A real-valued random variable \(X\) is said to be symmetric if \(X\) and \(-X\) have the same distribution.
\end{definition}
