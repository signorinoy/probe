\chapter{Graphical Lasso}
Before deriving the logarithmic likelihood function, we first introduce the concept of sample covariance matrix, inverse covariance matrix, and probability density function of multivariate normal distribution.
Let $X$ be a $p$-dimensional random vector with mean $\mu$ and covariance matrix $\Sigma$, then it follows multivariate normal distribution (also known as normal distribution): $$
	X \sim N(\mu, \Sigma)
$$ Its probability density function is: $$
	f_X(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^{\top} \Sigma^{-1} (x-\mu)\right)
$$ where $|\Sigma|$ represents the determinant of the covariance matrix.
The sample covariance matrix $S$ can be calculated from sample data, and its specific expression is: $$
	S = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^{\top}
$$ where $\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i$ represents the sample mean.
The determinant $|\Theta|$ of the inverse covariance matrix $\Theta$ represents the independence between variables. L1 regularization can make it equal to zero, so that sparse estimation results can be obtained. The inverse covariance matrix is the precision matrix of a specific multivariate normal distribution, and it is defined as: $$
	\Theta = (\theta_{ij}) = \Sigma^{-1}
$$
The derivation of the logarithmic likelihood function models the covariance matrix and inverse covariance matrix. The likelihood function is as follows: $$
	\begin{aligned}
		L(\Theta|x) & =\log P(x|\Theta)                                                                                                                        \\
		            & =\log \prod_{i=1}^{n} P(x_i|\Theta)                                                                                                      \\
		            & =\sum_{i=1}^{n} \log P(x_i|\Theta)                                                                                                       \\
		            & =\sum_{i=1}^{n} \left( -\frac{p}{2} \log(2 \pi ) - \frac{1}{2} \log |\Sigma| -\frac{1}{2} (x_i-\mu)^{\top} \Sigma^{-1} (x_i-\mu) \right) \\
		            & =-\frac{np}{2} \log2\pi -\frac{n}{2} \log |\Sigma| -\frac{1}{2} \sum_{i=1}^{n} (x_i-\mu)^{\top} \Sigma^{-1} (x_i-\mu)                    \\
		            & =-\frac{np}{2} \log2\pi -\frac{n}{2} \log |\Theta| -\frac{1}{2} \sum_{i=1}^{n} (x_i-\mu)^{\top} \Theta (x_i-\mu)
	\end{aligned}
$$ Taking the negative value of $L(\Theta|x)$ gives the logarithmic likelihood function, which is: $$
	\begin{aligned}
		\log L(\Theta|x) & =-L(\Theta|x)                                                                                                   \\
		                 & =\frac{np}{2} \log2\pi +\frac{n}{2} \log |\Theta| +\frac{1}{2} \sum_{i=1}^{n} (x_i-\mu)^{\top} \Theta (x_i-\mu)
	\end{aligned}
$$ where $\mu$ is the sample mean and $\Sigma$ is obtained from $\Theta$. The Graphical Lasso loss function obtained by regularization combines this logarithmic likelihood term with the L1 regularization term.
