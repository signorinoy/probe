\chapter{Graphical Lasso}
Before deriving the logarithmic likelihood function, we first introduce the concept of sample covariance matrix, inverse covariance matrix, and probability density function of multivariate normal distribution.
Let \(X\) be a \(p\)-dimensional random vector with mean \(\mu\) and covariance matrix \(\Sigma\), then it follows multivariate normal distribution (also known as normal distribution): \(X \sim N(\mu, \Sigma)\). Its probability density function is:
\begin{equation*}
	f_X(x) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^{\top} \Sigma^{-1} (x-\mu)\right),
\end{equation*}
where \(|\Sigma|\) represents the determinant of the covariance matrix.
The sample covariance matrix \(S\) can be calculated from sample data, and its specific expression is:
\begin{equation*}
	S = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^{\top}
\end{equation*}
where \(\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i\) represents the sample mean.
The determinant \(|\Theta|\) of the inverse covariance matrix \(\Theta\) represents the independence between variables. L1 regularization can make it equal to zero, so that sparse estimation results can be obtained. The inverse covariance matrix is the precision matrix of a specific multivariate normal distribution, and it is defined as: \(\Theta = (\theta_{ij}) = \Sigma^{-1}\).
The derivation of the logarithmic likelihood function models the covariance matrix and inverse covariance matrix. The likelihood function is as follows:
\begin{equation*}
	\begin{aligned}
		L(\Theta|x) & =\log P(x|\Theta)                                                                                                                        \\
		            & =\log \prod_{i=1}^{n} P(x_i|\Theta)                                                                                                      \\
		            & =\sum_{i=1}^{n} \log P(x_i|\Theta)                                                                                                       \\
		            & =\sum_{i=1}^{n} \left( -\frac{p}{2} \log(2 \pi ) - \frac{1}{2} \log |\Sigma| -\frac{1}{2} (x_i-\mu)^{\top} \Sigma^{-1} (x_i-\mu) \right) \\
		            & =-\frac{np}{2} \log2\pi -\frac{n}{2} \log |\Sigma| -\frac{1}{2} \sum_{i=1}^{n} (x_i-\mu)^{\top} \Sigma^{-1} (x_i-\mu)                    \\
		            & =-\frac{np}{2} \log2\pi -\frac{n}{2} \log |\Theta| -\frac{1}{2} \sum_{i=1}^{n} (x_i-\mu)^{\top} \Theta (x_i-\mu)
	\end{aligned}
\end{equation*}
Taking the negative value of \(L(\Theta|x)\) gives the logarithmic likelihood function, which is:
\begin{equation*}
	\begin{aligned}
		\log L(\Theta|x) & =-L(\Theta|x)                                                                                                   \\
		                 & =\frac{np}{2} \log2\pi +\frac{n}{2} \log |\Theta| +\frac{1}{2} \sum_{i=1}^{n} (x_i-\mu)^{\top} \Theta (x_i-\mu)
	\end{aligned}
\end{equation*}
where \(\mu\) is the sample mean and \(\Sigma\) is obtained from \(\Theta\). The Graphical Lasso loss function obtained by regularization combines this logarithmic likelihood term with the L1 regularization term.
