\chapter{Bayesian Inference}

\section{Bayes Estimator}

We shall look for some estimators that make the risk function \(R\left(\theta,\delta\right)\) small in some overall sense. There are two ways to solve it: minimize the average risk, and minimize the maximum risk.

This chapter will discuss the first method, also known as, Bayes Estimator.

\begin{definition}[Bayes Estimator]\label{def:bayes-estimator}
	The Bayes Estimator \(\delta\) with respect to \(\Lambda\) is minimizing the Bayes Risk of \(\delta\)
	\begin{equation}
		r\left(\Lambda, \delta\right)=\int R\left(\theta, \delta\right) \dif \Lambda\left(\theta\right)
	\end{equation}
	where \(\Lambda\) is the probability distribution.
\end{definition}

In Bayesian arguments, it is important to keep track of which variables are being conditioned. Hence, the notations are as follows:
\begin{itemize}
	\item The density of \(X\) will be denoted by \(X \sim f\left(x \mid \theta\right)\).
	\item The prior distribution will be denoted by \(\Pi \sim \pi\left(\theta \mid \lambda\right)\) or \(\Lambda \sim \gamma\left(\lambda\right)\), where \(\lambda\) is another parameter (sometimes called a hyperparameter).
	\item The posterior distribution, which calculates the conditional distributions as that of \(\theta\) given \(x\) and \(\lambda\), or \(\lambda\) given \(x\), which is denoted by \(\Pi \sim \pi\left(\theta \mid x, \lambda\right)\) or \(\Lambda \sim \gamma\left(\lambda \mid x\right)\), that is
	      \begin{equation}
		      \pi\left(\theta \mid x, \lambda\right) = \frac{f\left(x \mid \theta\right) \pi\left(\theta \mid \lambda\right)}{m\left(x \mid \lambda\right)},
	      \end{equation}
	      where marginal distributions \(m\left(x \mid \lambda\right) = \int f\left(x \mid \theta\right) \pi\left(\theta \mid \lambda\right) \dif \theta\).
\end{itemize}

\begin{theorem}\label{thm:bayes-definition}
	Let \(\Theta\) have distribution \(\Lambda\), and given \(\Theta=\theta\), let \(X\) have distribution \(P_{\theta}\). Suppose, the following assumptions hold for the problem of estimating \(g\left(\Theta\right)\) with non-negative loss function \(L\left(\theta,d\right)\),
	\begin{itemize}
		\item There exists an estimator \(\delta_0\) with finite risk.
		\item For almost all \(x\), there exists a value \(\delta_{\Lambda}\left(x\right)\) minimizing
		      \begin{equation}
			      E\{L[\Theta,\delta\left(x\right)] \mid X=x\}.
		      \end{equation}
	\end{itemize}
	Then, \(\delta_{\Lambda}\left(x\right)\) is a Bayes Estimator.
\end{theorem}
\begin{remark}
	Improper prior
\end{remark}

\begin{corollary}
	Suppose the assumptions of Theorem~\ref{thm:bayes-definition} hold.
	\begin{enumerate}
		\item If \(L\left(\theta,d\right)=[d-g\left(\theta\right)]^2\), then
		      \begin{equation}
			      \delta_{\Lambda}\left(x\right)=E[g\left(\Theta\right) \mid x].
		      \end{equation}
		\item If \(L\left(\theta,d\right)=w\left(\theta\right)[d-g\left(\theta\right)]^2\), then
		      \begin{equation}
			      \delta_{\Lambda}\left(x\right)=\frac{E[w\left(\theta\right)g\left(\Theta\right) \mid x]}{E[w\left(\theta\right) \mid x]}.
		      \end{equation}
		\item If \(L\left(\theta,d\right)=|d-g\left(\theta\right)|\), then \(\delta_\Lambda\left(x\right)\) is any median of the conditional distribution of \(\Theta\) given \(x\).
		\item If
		      \begin{equation*}
			      L(\theta, d)=\left\{
			      \begin{array}{l}
				      0 \text { when }|d-\theta| \leq c \\
				      1 \text { when }|d-\theta|>c
			      \end{array}
			      \right.,
		      \end{equation*}
		      then \(\delta_\Lambda\left(x\right)\) is the midpoint of the interval \(I\) of length \(2c\) which maxmizes \(P\left(\Theta\in I\mid x\right)\).
	\end{enumerate}
\end{corollary}

\begin{proof}

\end{proof}

\begin{theorem}
	Necessary condition for Bayes Estimator
\end{theorem}

Methodologies have been developed to deal with the difficulty which sometimes incorporates frequentist measures to assess the choice of \(\Lambda\).

\begin{itemize}
	\item Empirical Bayes.
	\item Hierarchical Bayes.
	\item Robust Bayes.
	\item Objective Bayes.
\end{itemize}

\subsection{Single-Prior Bayes}

The Single-Prior Bayes model in a general form as
\begin{equation}
	\begin{aligned}
		X\mid\theta      & \sim f\left(x\mid\theta\right),         \\
		\Theta\mid\gamma & \sim \pi\left(\theta\mid\lambda\right),
	\end{aligned}
	\label{eq:single-prior-bayes}
\end{equation}
where we assume that the functional form of the prior and the value of \(\lambda\) is known (we will write it as \(\gamma=\gamma_0)\).

Given a loss function \(L\left(\theta,d\right)\), we would then determine the estimator that minimizes
\begin{equation}
	\int L\left(\theta,d\left(x\right)\right)\pi\left(\theta\mid x\right)\dif\theta,
\end{equation}
where \(\pi\left(\theta\mid x\right)\) is posterior distribution given by
\begin{equation*}
	\pi\left(\theta\mid x\right)=\frac{f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma_0\right)}{\int f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma_0\right)\dif\theta}.
\end{equation*}

In general, this Bayes estimator under squared error loss is given by
\begin{equation}
	E\left(\Theta\mid x\right) = \frac{\int\theta f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma_0\right)\dif\theta}{\int f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma_0\right)\dif\theta}.
\end{equation}

\begin{example}
	Consider
	\begin{equation*}
		\begin{aligned}
			X_i    & \stackrel{\text{i.i.d}}{\sim}N(\mu,\Gamma^{-1}),\quad i=1,2,\ldots,n \\
			\mu    & \sim N(0,1),                                                         \\
			\Gamma & \sim\text{Gamma}(2,1),
		\end{aligned}
	\end{equation*}
	calculate the Single-Prior Bayes estimator under squared error loss.
\end{example}

\begin{proof}
	\begin{equation*}
		\begin{aligned}
			p\left(\textbf{X}\mid\mu,\Gamma\right) & =\Gamma^n(2\pi)^{-\frac{n}{2}}\exp\left[-2\Gamma^2\sum_{i=1}^{n}(x_i-\mu)^2\right], \\
			p(\mu)                                 & =\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\mu^2}{2}\right),                            \\
			p(\Gamma)                              & =\frac{1}{\Gamma(2)}\Gamma\exp\left(-\Gamma\right).
		\end{aligned}
	\end{equation*}

	Therefore,
	\begin{equation*}
		h\left(\textbf{X},\mu,\Gamma\right)=C\Gamma^n\exp\left[-2\Gamma^2\sum_{i=1}^{n}(x_i-\mu)^2\right]\exp\left(-\frac{\mu^2}{2}\right)\Gamma\exp\left(-\Gamma\right),
	\end{equation*}
	where \(C=\frac{(2\pi)^{-\frac{n+1}{2}}}{\Gamma(2)}\).

	For \(\mu\), we have
	\begin{equation*}
		\pi\left(\mu\mid\textbf{X},\Gamma\right)=\frac{h\left(\textbf{X},\mu,\Gamma\right)}{p(\mu\mid\textbf{X})}
	\end{equation*}
\end{proof}

For exponential families

\begin{theorem}

\end{theorem}

\subsection{Hierarchical Bayes}

In a Hierarchical Bayes model, rather than specifying the prior distribution as a single function, we specify it in a \textbf{hierarchy}. Thus, the Hierarchical Bayes model in a general form as
\begin{equation}
	\begin{aligned}
		X\mid\theta      & \sim f\left(x\mid\theta\right),         \\
		\Theta\mid\gamma & \sim \pi\left(\theta\mid\lambda\right), \\
		\Gamma           & \sim \psi\left(\gamma\right),
	\end{aligned}
	\label{eq:hierarchical-bayes}
\end{equation}
where we assume that \(\psi\left(\cdot\right)\) is known and not dependent on any other unknown hyperparameters.

\begin{remark}
	We can continue this hierarchical modeling and add more stages to the model, but this is not then done in practice.
\end{remark}

Given a loss function \(L\left(\theta,d\right)\), we would then determine the estimator that minimizes
\begin{equation} \label{eq:hierarchical-bayes-estimator}
	\int L\left(\theta,d\left(x\right)\right)\pi\left(\theta\mid x\right)\dif\theta,
\end{equation}
where \(\pi\left(\theta\mid x\right)\) is posterior distribution given by
\begin{equation*}
	\pi\left(\theta\mid x\right)=\frac{\int f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma\right)\psi\left(\gamma\right)\dif\gamma}{\int\int f\left(x\mid\theta\right)\pi\left(\theta\mid\gamma\right)\psi\left(\gamma\right)\dif\theta\dif\gamma}.
\end{equation*}

\begin{remark}
	The posterior distribution can also be written as
	\begin{equation*}
		\pi\left(\theta\mid x\right)=\int\pi\left(\theta\mid x,\gamma\right)\pi\left(\gamma\mid x\right)\dif\gamma,
	\end{equation*}
	where \(\pi\left(\gamma\mid x\right)\) is the posterior distribution of \(\Gamma\), unconditional on \(\theta\). The equation~\ref{eq:hierarchical-bayes-estimator} can be written as
	\begin{equation*}
		\int L\left(\theta,d\left(x\right)\right)\pi\left(\theta\mid x\right)\dif\theta = \int\left[\int L\left(\theta,d\left(x\right)\right)\pi\left(\theta\mid x,\gamma\right)\dif\theta\right]\pi\left(\gamma\mid x\right)\dif\gamma.
	\end{equation*}
	which shows that \textbf{the Hierarchical Bayes estimator can be thought of as a mixture of Single-Prior estimators}.
\end{remark}

\begin{example}[Poisson Hierarchy]
	Consider
	\begin{equation}
		\begin{aligned}
			X_i\mid\lambda & \stackrel{\text{i.i.d}}{\sim} \text{Poisson}\left(\lambda\right),\quad i=1,2\ldots,n \\
			\lambda\mid b  & \sim \text{Gamma}\left(a,b\right), \text{a known},                                   \\
			\frac{1}{b}    & \sim \text{Gamma}\left(k,\tau\right),                                                \\
		\end{aligned}
	\end{equation}
	calculate the Hierarchical Bayes estimator under squared error loss.
\end{example}

\begin{theorem}
	For the Hierarchical Bayes model (\ref{eq:hierarchical-bayes}),
	\begin{equation}
		K\left[\pi\left(\lambda\mid x\right),\psi\left(\lambda\right)\right] < K\left[\pi\left(\theta\mid x\right),\pi\left(\theta\right)\right],
	\end{equation}
	where \(K\) is the Kullback-Leibler information for discrimination between two densities.
\end{theorem}

\begin{proof}

\end{proof}

\begin{remark}

\end{remark}

\subsection{Empirical Bayes}

\subsection{Bayes Prediction}
