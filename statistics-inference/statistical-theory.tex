\chapter{Statistical Theory}

\section{Populations and Samples}

\section{Statistics}

\subsection{Sufficient Statistics}

\begin{definition}[Sufficient Statistics]
	A statistic \(T\) is said to be sufficient for \(X\), or for the family \(\mathcal{P}=\left\{P_{\theta}, \theta \in \Omega\right\}\) of possible distributions of \(X\), or for \(\theta\), if the conditional distribution of \(X\) given \(T=t\) is independent of \(\theta\) for all \(t\).
\end{definition}

\begin{theorem}[Fisher-Neyman Factorization Theorem]
	If the probability density function is \(p_{\theta}(x)\), then \(T\) is sufficient for \(\theta\) if and only if nonnegative functions \(g\) and \(h\) can be found such that
	\begin{equation*}
		p_{\theta}(x)=h(x)g_{\theta}[T(x)].
	\end{equation*}
\end{theorem}

\begin{proof}

\end{proof}

\subsection{Complete Statistics}

\begin{definition}[Complete Statistics]
	A statistic \(T\) is said to be complete, if \(Eg(T)=0\) for all \(\theta\) and some function \(g\) implies that \(P(g(T)=0\mid\theta)=1\) for all \(\theta\).
\end{definition}

\section{Estimators}

\begin{definition}[Estimator]\label{def:estimator}
	An estimator is a real-valued function defined over the sample space, that is
	\begin{equation}
		\delta:\textbf{X}\rightarrow\bbR.
	\end{equation}
	It is used to estimate an estimand, \(\theta\), a real-valued function of the parameter.
\end{definition}

\subsection*{Unbiasedness}

\begin{definition}[Unbiasedness]
	An estimator \(\hat{\theta}\) of \(\theta\) is unbiased if
	\begin{equation}
		E\hat{\theta}=\theta,\quad\forall\theta\in\Theta.
	\end{equation}
\end{definition}

\begin{remark}
	\begin{itemize}
		\item Unbiased estimators of \(\theta\) may not exist.
		\item
	\end{itemize}
\end{remark}

\begin{example}[Nonexistence of Unbiased Estimator]

\end{example}

\subsection*{Consistency}

\begin{definition}[Consistency]
	An estimator \(\hat{\theta}_n\) of \(\theta\) is consistent if
	\begin{equation}
		\lim_{n\rightarrow\infty}P\left(\left|\hat{\theta}_n-\theta\right|>\varepsilon\right)=0,\quad\forall\varepsilon>0,
	\end{equation}
	that is,
	\begin{equation}
		\hat{\theta}_{n}\stackrel{p}{\rightarrow}\theta.
	\end{equation}
\end{definition}

\begin{example}[Consistency of Sample Moments]

\end{example}

\begin{remark}
	\begin{enumerate}
		\item Unbiased But Consistent
		\item Biased But Not Consistent
	\end{enumerate}
\end{remark}

\subsection*{Asymptotic Normality}

\begin{definition}[Asymptotic Normality]
	An estimator \(\hat{\theta}_n\) of \(\theta\) is asymptotic normality if
	\begin{equation}
		\sqrt{n}\left(\hat{\theta}-\theta\right)\stackrel{d}{\rightarrow}N\left(0,\sigma_{\theta}^{2}\right).
	\end{equation}
\end{definition}

\subsection*{Efficiency}

\begin{definition}[Efficiency]

\end{definition}

\subsection*{Robustness}

\begin{definition}[Robustness]

\end{definition}
