\chapter{Minimax Theory}

\section{Fano's Inequality}

Let $X\sim P_{\theta}$, $\theta\in\Theta_{0}\subset\Theta$, in which $\Theta_{0}$ are assumed to be finithe , e.g, $\left\{\theta_{1},\ldots,\theta_{M}\right\}$, and $\theta$ uniformly distributed on $\Theta_{0}$. Let $\hat{\theta}$ be an estimator of $\theta$ based on $X$, Then
\begin{equation}
	P\left(\theta\neq\hat{\theta}\right)=\frac{1}{M}\sum_{1}^{M}P_{\theta_{i}}\left(\hat{\theta}\neq\theta_{i}\right)\geq 1-\frac{I\left(\theta,X\right)+\log 2}{\log M}.
\end{equation}

Question: How to upper bound $I\left(\theta, X\right)$?

There are various ways to do that, one earlier bond is
\begin{equation}
	I\left(\theta,X\right)\leq\frac{1}{M^{2}}\sum_{i=1}^{M}\sum_{j=1}^{M}D_{KL}\left(P_{\theta_{i}}\|\theta_{j}\right).
\end{equation}

To use such a bound needs to be very careful with the construction of $\Theta_{0}$. Alternatively, insight from the information theory may provide another method to do it in a way that takes advantage of known metric entropy ???.

Typically, $\Theta_{0}$ is a subset of $\Theta$.

Original problem $X\sim P_{\theta}$, $\theta\in\Theta$. Here $\theta$ can be a finite dimension or infinite dimension (e.g., $\theta=f(x)$, pdf of $X$).

Let $\pi(\theta)$ be a prior distribution on $\Theta$. To apply Fano's Inequality, we need to choose $\Theta$ finite. Regardlessly, we want a general bound on $I\left(\theta; X\right)$, in which $\theta\sim\pi$.

Recall
\begin{equation}
	H(X)=\left\{\begin{array}{ll}
		-\sum_{i}p_{i}\log p_{i}  & \text{discrete} \\
		-\int p(x)\log p(x)\dif x & \text{contious}
	\end{array}\right.
\end{equation}

Given the ?? Prof P. Shannon ?? achieves (within one bit) the lower bound on the expected code length of any prefix code, with code length $\log\frac{1}{p_{i}}$ (ignoring rounding up), which is $H(X)$.

If we mistakenly use $q$ to code, then the expected extra bits are (also called redundancy)
\begin{equation}
	\sum_{i}p_{i}\log\frac{1}{q_{i}}-\sum_{i}p_{i}\log\frac{1}{q_{i}}=\sum_{i}p_{i}\log\frac{p_{i}}{q_{i}}\geq 0.
\end{equation}

Bayes misfu?? mis?? the Bayes Redundancy. Let $q$ be any pdf, redundancy at $\theta$ is
\begin{equation}
	\int f(x,\theta)\log\frac{f(x,\theta)}{q(x)}\dif x,
\end{equation}
where $f(x,\theta)$ is pdf of $X$. Bayes redundancy is
\begin{equation}
	\begin{aligned}
		  & \int_{\Theta}\left(\int f(x,\theta)\log\frac{f(x,\theta)}{q(x)}\dif x\right)\cdot\pi(\theta)\dif\theta          \\
		= & \int\int_{\Theta}f(x,\theta)\log\frac{\pi(\theta)f(x,\theta)}{\pi(\theta)q(x)}\cdot\pi(\theta)\dif\theta\dif x,
	\end{aligned}
\end{equation}
Let $q^{*}(x)=\int_{\Theta}f(x,\theta)\pi(\theta)\dif\theta$,
\begin{equation}
	\begin{aligned}
		= & \int\int_{\Theta}f(x,\theta)\log\frac{\pi(\theta)f(x,\theta)}{\pi(\theta)q^{*}(x)}\cdot\pi(\theta)\dif\theta\dif x \\
		  & \qquad+\int\int_{\Theta}f(x,\theta)\log\frac{q^{*}(x)}{q(x)}\pi(\theta)\dif\theta\dif x,
	\end{aligned}
\end{equation}
in which, the first part is the Bayes redundancy of $q^{*}$, and the second part
\begin{equation}
	\begin{aligned}
		= & \int\log\frac{q^{*}(x)}{q(x)}\left(\int_{\Theta}f(x,\theta)\pi(\theta)\dif\theta\right)\dif x \\
		= & \int\log\frac{q^{*}(x)}{q(x)}q^{*}(x)\dif x\geq 0.
	\end{aligned}
\end{equation}
Thus Bayes redundancy of $q^{*}$ is $I\left(\theta;X\right)$.

Our approach is to provide a sensible upper bound on $I\left(\theta; X\right)$, that is not specific to the choice of $\Theta_{0}$. Rather, it reflects the native of the $\{p_{\theta},\theta\in\Theta\}$ (or a subset of it) more.

Suppose we have i.i.d observations $X_{1},X_{2},\ldots X_{n}\sim p_{\theta}$, let
\begin{equation}
	d_{k}(p,q)=\sqrt{D(p\|q)}=\sqrt{\int p(x)\log\frac{p(x)}{q(x)}\dif x},
\end{equation}
which is not a metric. Let $G_{\varepsilon}$ be an $\varepsilon$-cover of the family $\{p_{\theta},\theta\in\Theta\}$, i.e.,
\begin{equation}
	\forall\theta\in\Theta,\exists\theta\in G_{\varepsilon},\text{ s.t. }D\left(p_{\theta}\|p_{\theta^{\prime}}\right)\leq\varepsilon^{2}.
\end{equation}
Let $M=\left|G_{\varepsilon}\right|$, $q(x_{1},\ldots,x_{n})=\frac{1}{M}\sum_{i=1}^{M}p_{\theta_{i}}^{n}$ (centroid) and $p_{\theta_{i}}^{n}=p_{\theta_{i}}(x_{1})\ldots p_{\theta_{i}}(x_{n})$. Then,
\begin{equation}
	\begin{aligned}
		D_{\text{KL}}\left(p_{\theta}^{n}\|q\right)= & \int p_{\theta}^{n}\log\frac{p_{\theta}^{n}}{\frac{1}{M}\sum_{i=1}^{M}p_{\theta_{i}}^{n}}\dif x \\
		\leq                                         & \int p_{\theta}^{n}\log\frac{p_{\theta}^{n}}{\frac{1}{M}p_{\theta_{i}}^{n}}\dif x               \\
		=                                            & \log M+\inf_{i}\int p_{\theta}^{n}\log\frac{p_{\theta}^{n}}{p_{\theta_{i}}^{n}}\dif x           \\
		=                                            & log M+\inf_{i}D_{\text{KL}}\left(p_{\theta}\|p_{\theta_{i}}\right)                              \\
		\leq                                         & \log M+n\varepsilon^{2}.
	\end{aligned}
\end{equation}

This holds for all $\theta\in\Theta$. If we have a subset $\Theta$ and for any prior $\pi$, we have
\begin{equation}
	I\left(\theta;X^{n}\right)\leq\log|G_{\varepsilon}|+n\varepsilon^{2}.
\end{equation}

\begin{gather*}
	D_{\text{KL}}\left(p_{\theta_{i}}^{n}\|q\right)\leq\log M,\quad 1\leq i \leq M,\\
	D_{\text{KL}}\left(p_{\theta}^{n}\|q\right)\leq\log M+n\varepsilon^{2},\quad\forall\theta\in\Theta.
\end{gather*}

If $D_{\text{KL}}\left(p_{\theta_{i}}\|p_{\theta_{j}}\right)\geq\eta^{2}>0$, then $D_{\text{KL}}\left(p_{\theta_{i}}^{n}\|p_{\theta_{j}}^{n}\right)\geq n\eta^{2}$. But with the centroid density $q$, we have
\begin{equation*}
	D_{\text{KL}}\left(p_{\theta_{i}}^{n}\|q\right)\leq\log M.
\end{equation*}
Then, we have the interesting situation that $D_{\text{KL}}\left(p_{\theta_{i}}^{n}\|p_{\theta_{j}}^{n}\right)$ are very large for $i\neq j$, yet $D_{\text{KL}}\left(p_{\theta_{i}}^{n}\|q\right)$ are small. Relaxing speaking!

\section{Minimax Rate}

Consider a loss function $l(\theta,\theta^{\prime})$

\begin{theorem}
	Suppose on a finite set $\Theta_{0}\subset\Theta$, we have
	\begin{equation}
		\min_{\theta_{i}\neq\theta_{j}}l(\theta_{i},\theta_{j})\geq\Delta>0,
	\end{equation}
	for any $\theta_{i}\neq\theta_{j}\in\Theta_{0}$ and $\theta\in\Theta$, we have
	\begin{equation}
		l(\theta_{i},\theta)+l(\theta_{j},\theta)\geq c\Delta,
	\end{equation}
	for some constant $c>0$. Then
	\begin{equation}
		\inf_{\hat{\theta}}\sup_{\theta\in\Theta}\bbE_{\theta}l\left(\theta,\hat{\theta}\right)\geq\frac{c\Delta}{2}\left(1-\frac{V_{k}(\varepsilon)+n\varepsilon^{2}+\log 2}{\log|\Theta_{0}|}\right),
	\end{equation}
	where $V_{k}(\varepsilon)$ is the covering entropy of $\{p_{\theta},\theta\in\Theta\}$ under $d_{\text{KL}}$.
\end{theorem}

For $f$, $g$ with different supports, we possibly have $D(f\|g)=\infty$, suppose density are w.r.t a probability measure $\mu$. Given original observations $X_{1},\ldots,X_{n}\sim f\in\mathcal{F}$, let $Y_{1},\ldots,Y_{n}$ be i.i.d uniform w.r.t $\mu$, and $V_{1},\ldots,V_{n}$ be coin flips. Suppose
\begin{equation*}
	Z_{i}=\left\{\begin{array}{ll}
		X_{i}, & \text{ if }V_{i}=1, \\
		Y_{i}, & \text{ if }V_{i}=0. \\
	\end{array}\right.
\end{equation*}
Then, $Z_{1},\ldots,Z_{n}\sim\frac{f}{2}+\frac{1}{2}$.

\begin{proof}
	Let
	\begin{equation*}
		\tilde{\theta}=\underset{\theta_{i}\in\Theta_{0}}{\arg\min}l(\theta_{i},\hat{\theta}).
	\end{equation*}
	Then, we have $\theta\neq\tilde{\theta}$, we know
	\begin{equation*}
		l(\theta,\hat{\theta})\geq l(\tilde{\theta},\hat{\theta}),
	\end{equation*}
	and
	\begin{equation*}
		l(\theta,\hat{\theta})+l(\tilde{\theta},\hat{\theta})\geq c\Delta.
	\end{equation*}
	Consequently, $l(\theta,\hat{\theta})\geq\frac{c\Delta}{2}$. Thus,
	\begin{equation*}
		\inf_{\hat{\theta}}\sup_{\theta\in\Theta}\bbE_{\theta}l(\theta,\hat{\theta})\geq\inf_{\hat{\theta}}\sup_{\theta\in\Theta_{0}}\bbE_{\theta}l(\theta,\hat{\theta}).
	\end{equation*}
\end{proof}

\section{Applications}

\begin{enumerate}
	\item Consider a fixed design regression with independent observations:
	      \begin{equation*}
		      Y_{i}=u\left(X_{i}\right)+e_{i},\quad 1\leq i\leq n,
	      \end{equation*}
	      where $x_{i}=i/n$, $e_{i}\sim N(0,1)$ and $\mathcal{U}$ consists of all functions $g$ on $[0,1]$ that are uniformly bounded between $-A$ and $A$ for some positive constant $A$ and $|g(x)-g(y)|\leq L|x-y|$ for some constant $L>0$. The loss function of interest is $\ell(u,\hat{u})=\int_{0}^{1}(u(x)-\hat{u}(x))^{2}\,\mathrm{d}x$. Show that
	      \begin{equation*}
		      \inf_{\hat{u}}\sup_{u\in\mathcal{U}}E_{u}\ell(u,\hat{u})\asymp n^{-2/3}.
	      \end{equation*}
	      Note that some results given in the lectures on regression may not be directly applicable because here we deal with a fixed design. You may consider a piecewise constant estimator for upper bounding and you may use the fact that $\mathcal{U}$ has $L_{2}$ metric entropy order $1/\epsilon$.
	      \begin{proof}
		      Since $X$ are fixed design, then $y_{i}\sim N(u(x_{i}),1), i=1,2,\ldots,n$, which are independent but no longer i.i.d.  Let $P_{u}$ denotes the distribution of $Y$ with regression function $u$. Thus,
		      \begin{equation*}
			      D_{\text{KL}}\left(p_{u}^{n}\|p_{v}^{n}\right)=\frac{1}{2}\sum_{i=1}^{n}\left[u(x_{i})-v(x_{i})\right]^{2},
		      \end{equation*}
		      where $p_{u}^{n}(\mathbf{y})=\prod_{i=1}^{n}p_{u}(y_{i})$, and $p_{v}^{n}(\mathbf{y})=\prod_{i=1}^{n}p_{v}(y_{i})$.

		      \textbf{Lower Bound}:
	      \end{proof}
	\item Consider the collection $\mathcal{A}=\left\{(a,b):-\infty<a<b<\infty\right\}$ comprising sets in the real number line. Show its $\mathrm{VC}$ dimension is $2$.
	      \begin{proof}
		      Since we have $S_{\mathcal{A}}(n)=\frac{n(n+1)}{2}+1$, it follows that the $\mathrm{VC}$ dimension of $\mathcal{A}$ is $2$.
	      \end{proof}
\end{enumerate}
