\chapter{Introduction}

\section{Populations and Samples}

\section{Statistics}

\subsection{Sufficient Statistics}

\begin{definition}[Sufficient Statistics]
    A statistic $T$ is said to be sufficient for $X$, or for the family $\mathcal{P}=\left\{P_{\theta}, \theta \in \Omega\right\}$ of possible distributions of $X$, or for $\theta$, if the conditional distribution of $X$ given $T=t$ is independent of $\theta$ for all $t$.
\end{definition}

\begin{theorem}[Fisherâ€“Neyman Factorization Theorem]
    If the probability density function is $p_{\theta}(x)$, then $T$ is sufficient for $\theta$ if and only if nonnegative functions $g$ and $h$ can be found such that
    \begin{equation*}
        p_{\theta}(x)=h(x)g_{\theta}[T(x)].
    \end{equation*}
\end{theorem}

\begin{proof}

\end{proof}

\subsection{Complete Statistics}

\begin{definition}[Complete Statistics]
    A statistic $T$ is said to be complete, if $Eg(T)=0$ for all $\theta$ and some function $g$ implies that $P(g(T)=0\mid\theta)=1$ for all $\theta$.
\end{definition}

\section{Estimators}

\begin{definition}[Estimator] \label{def:estimator}
    An estimator is a real-valued function defined over the sample space, that is
    \begin{equation}
        \delta:\textbf{X}\rightarrow\mathbb{R}.
    \end{equation}
    It is used to estimate an estimand, $\theta$, a real-valued function of the parameter.
\end{definition}

\subsection*{Unbiasedness}

\begin{definition}[Unbiasedness]
    An estimator $\hat{\theta}$ of $\theta$ is unbiased if
    \begin{equation}
        E\hat{\theta}=\theta,\quad\forall\theta\in\Theta.
    \end{equation}
\end{definition}

\begin{remark}
    \begin{itemize}
        \item Unbiased estimators of $\theta$ may not exist.
        \item
    \end{itemize}
\end{remark}

\begin{example}[Nonexistence of Unbiased Estimator]

\end{example}

\subsection*{Consistency}

\begin{definition}[Consistency]
    An estimator $\hat{\theta}_n$ of $\theta$ is consistent if
    \begin{equation}
        \lim_{n\rightarrow\infty}P\left(\left|\hat{\theta}_n-\theta\right|>\varepsilon\right)=0,\quad\forall\varepsilon>0,
    \end{equation}
    that is,
    \begin{equation}
        \hat{\theta}_{n}\stackrel{p}{\rightarrow}\theta.
    \end{equation}
\end{definition}

\begin{example}[Consistency of Sample Moments]
    
\end{example}

\begin{remark}
    \begin{enumerate}
        \item Unbiased But Consistent
        \item Biased But Not Consistent
    \end{enumerate}
\end{remark}

\subsection*{Asymptotic Normality}

\begin{definition}[Asymptotic Normality]
    An estimator $\hat{\theta}_n$ of $\theta$ is asymptotic normality if
    \begin{equation}
        \sqrt{n}\left(\hat{\theta}-\theta\right)\stackrel{d}{\rightarrow}N\left(0,\sigma_{\theta}^{2}\right).
    \end{equation}
\end{definition}

\subsection*{Efficiency}

\begin{definition}[Efficiency]

\end{definition}

\subsection*{Robustness}

\begin{definition}[Robustness]

\end{definition}
