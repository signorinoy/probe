\chapter{Multivariate Extensions}

\section{Multivariate Distributions}

\subsection{Multivariate Normal Distribution}

\begin{definition}[Multivariate Normal Distribution]
	The multivariate normal distribution of a $p$-dimensional random vector $\bfX$ can be written as:
	\begin{equation*}
		\bfX\sim\mcN(\bfmu,\bfSigma)
	\end{equation*}
	where $\bfmu$ is a $p$-dimensional mean vector and $\bfSigma$ is a $p\times p$ covariance matrix. Furthermore, the probability density function of $\bfX$ is:
	\begin{equation*}
		p(\bfX)=\frac{1}{(2\pi)^{p/2}|\bfSigma|^{1/2}}\exp\left(-\frac{1}{2}(\bfX-\bfmu)^{\top}\bfSigma^{-1}(\bfX-\bfmu)\right).
	\end{equation*}
\end{definition}

\begin{theorem}
	\label{thm:mvn-properties}
	Suppose $\bfX\sim\mcN(\bfmu,\bfSigma)$, then:
	\begin{enumerate}
		\item $\bfSigma^{-1/2}(\bfX-\bfmu)\sim\mcN(\bfzero,\bfI)$.
		\item $(\bfX-\bfmu)^{\top}\bfSigma^{-1}(\bfX-\bfmu)\sim\chi^{2}_{p}$.
	\end{enumerate}
\end{theorem}

\subsection{Wishart Distribution}

\begin{definition}[Wishart Distribution]
	The Wishart distribution is a generalization of the chi-squared distribution to multiple dimensions. If $\bfZ$ is a $p\times n$ matrix with each column drawn from a multivariate normal distribution $\mcN(\bfzero,\bfSigma)$, then the quadratic form $\bfX$ has a Wishart distribution (with parameters $\bfSigma$, and $n$):
	\begin{equation*}
		\bfX=\bfZ\bfZ^{\top}\sim W_{p}(\bfSigma,n).
	\end{equation*}
	Furthermore, the probability density function of $\bfX$ is:
	\begin{equation*}
		p(\bfX)=\frac{|\bfX|^{(n-p-1)/2}\exp\left(-\frac{1}{2}\tr(\bfSigma^{-1}\bfX)\right)}{2^{np/2}|\bfSigma|^{n/2}\Gamma_{p}(n/2)}
	\end{equation*}
\end{definition}

\subsection{Hotelling's T-squared Distribution}

\begin{definition}[Hotelling's $T^{2}$ Distribution]
	If the vector $\bfd$ is Gaussian multivariate-distributed with zero mean and unit covariance matrix $\mcN\left(\bfzero_p,\bfI_{p}\right)$ and $\bfM$ is a $p\times p$ matrix with unit scale matrix and $m$ degrees of freedom with a Wishart distribution $W\left(\bfI_{p},m\right)$, then the quadratic form $X$ has a Hotelling distribution (with parameters $p$ and $m$):
	\begin{equation*}
		X=m\bfd^{\top}\bfM^{-1}\bfd\sim T^2(p,m) .
	\end{equation*}

	Furthermore, if a random variable $X$ has Hotelling's $T$-squared distribution, $X \sim T_{p,m}^2$, then:
	\begin{equation*}
		\frac{m-p+1}{pm}X\sim F_{p,m-p+1}
	\end{equation*}
	where $F_{p,m-p+1}$ is the $F$-distribution with parameters $p$ and $m-p+1$.
\end{definition}

\section{Convergence of Random Vectors}

Let $\bfX^{(n)}$ be a sequence of random vectors with cdf $H_n$ converging in law to $\bfX$ with cdf $H$. One then often needs to know whether for some set $S$ in $R^k$,
\begin{equation}
	\label{eq:convergence-of-random-vectors}
	P\left(\bfX^{(n)} \in S\right) \rightarrow P(\bfX \in S) .
\end{equation}

That \eqref{eq:convergence-of-random-vectors} need not be true for all $S$ is seen from the case $k=1$, $S=\{x:x\leq a\}$. Then \eqref{eq:convergence-of-random-vectors} can only be guaranteed when $a$ is a continuity point of $H$.
\begin{theorem}
	\label{thm:sufficient-condition-for-convergence-of-random-vectors}
	A sufficient condition for \eqref{eq:convergence-of-random-vectors} to hold is that
	\begin{equation*}
		P(\bfX\in\partial S)=0 .
	\end{equation*}
\end{theorem}

\begin{example}[Multinomial]

\end{example}

\begin{example}[Difference of Means]
	Let $X_{1},\ldots,X_m$ and $Y_{1},\ldots,Y_n$ be independently distributed according to distributions $F$ and $G$, with means $\xi$ and $\eta$ and finite variances $\sigma^2$ and $\tau^2$, respectively. Then
	\begin{equation*}
		\sqrt{m}(\bar{X}-\xi)\stackrel{d}{\rightarrow}\mcN\left(0,\sigma^2\right),\quad\sqrt{n}(\bar{Y}-\eta)\stackrel{d}{\rightarrow}\mcN\left(0,\tau^2\right).
	\end{equation*}
	If $\frac{m}{m+n} \rightarrow \lambda$ $(0<\lambda<1)$, it follows that
	\begin{gather*}
		\sqrt{m+n}(\bar{X}-\xi)=\sqrt{\frac{m+n}{m}}\sqrt{m}(\bar{X}-\xi)\stackrel{d}{\rightarrow}N\left(0\frac{\sigma^2}{\lambda}\right),\\
		\sqrt{m+n}(\bar{Y}-\eta)\stackrel{d}{\rightarrow}N\left(0,\frac{\tau^2}{1-\lambda}\right),
	\end{gather*}
	and hence that
	\begin{equation*}
		(\sqrt{m+n}(\bar{X}-\xi),\sqrt{m+n}(\bar{Y}-\eta)) \stackrel{d}{\rightarrow}(X,Y),
	\end{equation*}
	where $X$ and $Y$ are independent random variables with distributions $N\left(0, \frac{\sigma^2}{\lambda}\right)$ and $N\left(0, \frac{\tau^2}{1-\lambda}\right)$, respectively.
	Since $Y-X$ is a continuous function of $(X, Y)$, it follows that
	\begin{equation*}
		\sqrt{m+n}[(\bar{Y}-\bar{X})-(\eta-\xi)]\stackrel{d}{\rightarrow}N\left(0,\frac{\sigma^2}{\lambda}+\frac{\tau^2}{1-\lambda}\right),
	\end{equation*}
	or, equivalently, that
	\begin{equation*}
		\frac{(\bar{Y}-\bar{X})-(\eta-\xi)}{\sqrt{\frac{\sigma^2}{m}+\frac{\tau^2}{n}}}\stackrel{d}{\rightarrow}N(0,1).
	\end{equation*}

	More specifically, consider the probability
	\begin{equation*}
		P\{\sqrt{m+n}[(\bar{Y}-\bar{X})-(\eta-\xi)]\leq z\}.
	\end{equation*}
	By Theorem \ref{thm:sufficient-condition-for-convergence-of-random-vectors}, since $P(Y-X=z)=0$, it follows that
	\begin{equation*}
		P\{(Y-X) \leq z\}=\Phi\left(\frac{z}{\sqrt{\frac{\sigma^2}{\lambda}+\frac{\tau^2}{1-\lambda}}}\right).
	\end{equation*}
\end{example}

\begin{theorem}
	\label{thm:convergence-in-distribution-by-linear-combinations}
	A necessary and sufficient condition for
	\begin{equation*}
		(X_{1}^{(n)},\ldots,X_k^{(n)})\stackrel{d}{\rightarrow}(X_{1},\ldots,X_k),
	\end{equation*}
	is that for any constants $c_{1},\ldots,c_k$, we have
	\begin{equation*}
		\sum_{i=1}^{k}c_{i}X_{i}^{(n)}\stackrel{d}{\rightarrow}\sum_{i=1}^{k}c_{i}X_{i}.
	\end{equation*}
\end{theorem}

\begin{example}[Orthogonal Linear Combinations]
	Let $Y_{1},Y_{2},\ldots$ be i.i.d. with mean $\bbE\left(Y_{i}\right)=0$ and variance $\Var\left(Y_{i}\right)=\sigma^2$, and consider the joint distribution of the linear combinations
	\begin{equation*}
		X_{1}^{(n)}=\sum_{j=1}^{n}a_{nj}Y_{j},\quad X_{2}^{(n)}=\sum_{j=1}^{n}b_{nj}Y_{j}
	\end{equation*}
	satisfying the orthogonality conditions
	\begin{equation}
		\label{eq:orthogonality-conditions-2}
		\sum_{j=1}^{n}a_{nj}^{2}=\sum_{j=1}^{n}b_{nj}^2=1,\quad \sum_{j=1}^{n}a_{nj}b_{nj}=0.
	\end{equation}
	Then if
	\begin{equation*}
		\max_{j}a_{jn}^{2}\rightarrow 0,\quad\max_{j}b_{jn}^2\rightarrow 0 \text { as } n \rightarrow \infty .
	\end{equation*}
	it follows that
	\begin{equation*}
		(X_{1}^{(n)},X_{2}^{(n)})\stackrel{d}{\rightarrow}(X_{1},X_{2}),
	\end{equation*}
	with $(X_{1},X_{2})$ independently distributed, each according to the normal distribution $N\left(0,\sigma^2\right)$.
\end{example}

\begin{proof}
	To prove this result, it is by Theorem \ref{thm:convergence-in-distribution-by-linear-combinations} enough to show that
	\begin{equation*}
		c_{1} X_{1}^{(n)}+c_{2} X_{2}^{(n)}=\sum\left(c_{1} a_{nj}+c_{2} b_{nj}\right) Y_{j} \stackrel{d}{\rightarrow}c_{1} X_{1}+c_{2} X_{2},
	\end{equation*}
	where the distribution of $c_{1} X_{1}+c_{2} X_{2}$ is $N\left(0,\left[c_{1}^2+c_{2}^2\right] \sigma^2\right)$. The sum on the left side of (5.1.24) is of the form $\sum d_{nj} Y_{j}$ with
	\begin{equation*}
		d_{nj}=c_{1} a_{nj}+c_{2} b_{nj}
	\end{equation*}

	It follows from \eqref{eq:orthogonality-conditions-2} that
	\begin{equation*}
		\sum_{j=1}^{n}d_{nj}^2=c_{1}^2+c_{2}^2
	\end{equation*}
	furthermore
	\begin{equation*}
		\max d_{nj}^2\leq 2\max\left[c_{1}^2a_{nj}^2+c_{2}^2 b_{nj}^2\right]\leq 2\left[c_{1}^2 \max a_{nj}^2+c_{2}^2\max b_{nj}^2\right] .
	\end{equation*}
	Thus,
	\begin{equation*}
		\max_{j}d_{nj}^{2}/\sum_{j=1}^{n}d_{nj}^{2}\rightarrow 0.
	\end{equation*}

	According to Theorems \ref{thm:linear-combination-inid}, it thus follows that
	\begin{equation*}
		\sum d_{nj} Y_{j} \stackrel{d}{\rightarrow}N\left(0,\left(c_{1}^2+c_{2}^2\right) \sigma^2\right).
	\end{equation*}
	Thus, by Theorem \ref{thm:convergence-in-distribution-by-linear-combinations}, we have
	\begin{equation*}
		(X_{1}^{(n)},X_{2}^{(n)})\stackrel{d}{\rightarrow}(X_{1},X_{2}),
	\end{equation*}
	with $(X_{1},X_{2})$ independently distributed, each according to the normal distribution $N\left(0,\sigma^2\right)$.
\end{proof}
