\chapter{Multivariate Extensions}

\section{Multivariate Distributions}

\subsection{Multivariate Normal Distribution}

\begin{definition}[Multivariate Normal Distribution]
	The multivariate normal distribution of a \(p\)-dimensional random vector \(\bfX\) can be written as:
	\begin{equation*}
		\bfX\sim\mcN(\bfmu,\bfSigma)
	\end{equation*}
	where \(\bfmu\) is a \(p\)-dimensional mean vector and \(\bfSigma\) is a \(p\times p\) covariance matrix. Furthermore, the probability density function of \(\bfX\) is:
	\begin{equation*}
		p(\bfX)=\frac{1}{(2\pi)^{p/2}|\bfSigma|^{1/2}}\exp\left(-\frac{1}{2}(\bfX-\bfmu)^{\top}\bfSigma^{-1}(\bfX-\bfmu)\right).
	\end{equation*}
\end{definition}

\begin{theorem}\label{thm:mvn-properties}
	Suppose \(\bfX\sim\mcN(\bfmu,\bfSigma)\), then:
	\begin{enumerate}
		\item \(\bfSigma^{-1/2}(\bfX-\bfmu)\sim\mcN(\bfzero,\bfI)\).
		\item \((\bfX-\bfmu)^{\top}\bfSigma^{-1}(\bfX-\bfmu)\sim\chi^{2}_{p}\).
	\end{enumerate}
\end{theorem}

\subsection{Wishart Distribution}

\begin{definition}[Wishart Distribution]
	The Wishart distribution is a generalization of the chi-squared distribution to multiple dimensions. If \(\bfZ\) is a \(p\times n\) matrix with each column drawn from a multivariate normal distribution \(\mcN(\bfzero,\bfSigma)\), then the quadratic form \(\bfX\) has a Wishart distribution (with parameters \(\bfSigma\), and \(n\)):
	\begin{equation*}
		\bfX=\bfZ\bfZ^{\top}\sim W_{p}(\bfSigma,n).
	\end{equation*}
	Furthermore, the probability density function of \(\bfX\) is:
	\begin{equation*}
		p(\bfX)=\frac{|\bfX|^{(n-p-1)/2}\exp\left(-\frac{1}{2}\tr(\bfSigma^{-1}\bfX)\right)}{2^{np/2}|\bfSigma|^{n/2}\Gamma_{p}(n/2)}
	\end{equation*}
\end{definition}

\subsection{Hotelling's T-squared Distribution}

\begin{definition}[Hotelling's \(T^{2}\) Distribution]
	If the vector \(\bfd\) is Gaussian multivariate-distributed with zero mean and unit covariance matrix \(\mcN\left(\bfzero_p,\bfI_{p}\right)\) and \(\bfM\) is a \(p\times p\) matrix with unit scale matrix and \(m\) degrees of freedom with a Wishart distribution \(W\left(\bfI_{p},m\right)\), then the quadratic form \(X\) has a Hotelling distribution (with parameters \(p\) and \(m\)):
	\begin{equation*}
		X=m\bfd^{\top}\bfM^{-1}\bfd\sim T^2(p,m).
	\end{equation*}

	Furthermore, if a random variable \(X\) has Hotelling's \(T\)-squared distribution, \(X \sim T_{p,m}^2\), then:
	\begin{equation*}
		\frac{m-p+1}{pm}X\sim F_{p,m-p+1}
	\end{equation*}
	where \(F_{p,m-p+1}\) is the \(F\)-distribution with parameters \(p\) and \(m-p+1\).
\end{definition}

\section{Convergence of Random Vectors}

Let \(\bfX^{(n)}\) be a sequence of random vectors with cdf \(H_n\) converging in law to \(\bfX\) with cdf \(H\). One then often needs to know whether for some set \(S\) in \(R^k\),
\begin{equation}
	\label{eq:convergence-of-random-vectors}
	P\left(\bfX^{(n)} \in S\right) \rightarrow P(\bfX \in S).
\end{equation}

That~\eqref{eq:convergence-of-random-vectors} need not be true for all \(S\) is seen from the case \(k=1\), \(S=\{x:x\leq a\}\). Then~\eqref{eq:convergence-of-random-vectors} can only be guaranteed when \(a\) is a continuity point of \(H\).
\begin{theorem}\label{thm:sufficient-condition-for-convergence-of-random-vectors}
	A sufficient condition for~\eqref{eq:convergence-of-random-vectors} to hold is that
	\begin{equation*}
		P(\bfX\in\partial S)=0.
	\end{equation*}
\end{theorem}

\begin{example}[Multinomial]

\end{example}

\begin{example}[Difference of Means]
	Let \(X_{1},\ldots,X_m\) and \(Y_{1},\ldots,Y_n\) be independently distributed according to distributions \(F\) and \(G\), with means \(\xi\) and \(\eta\) and finite variances \(\sigma^2\) and \(\tau^2\), respectively. Then
	\begin{equation*}
		\sqrt{m}(\bar{X}-\xi)\stackrel{d}{\rightarrow}\mcN\left(0,\sigma^2\right),\quad\sqrt{n}(\bar{Y}-\eta)\stackrel{d}{\rightarrow}\mcN\left(0,\tau^2\right).
	\end{equation*}
	If \(\frac{m}{m+n} \rightarrow \lambda\) \((0<\lambda<1)\), it follows that
	\begin{gather*}
		\sqrt{m+n}(\bar{X}-\xi)=\sqrt{\frac{m+n}{m}}\sqrt{m}(\bar{X}-\xi)\stackrel{d}{\rightarrow}N\left(0\frac{\sigma^2}{\lambda}\right),\\
		\sqrt{m+n}(\bar{Y}-\eta)\stackrel{d}{\rightarrow}N\left(0,\frac{\tau^2}{1-\lambda}\right),
	\end{gather*}
	and hence that
	\begin{equation*}
		(\sqrt{m+n}(\bar{X}-\xi),\sqrt{m+n}(\bar{Y}-\eta)) \stackrel{d}{\rightarrow}(X,Y),
	\end{equation*}
	where \(X\) and \(Y\) are independent random variables with distributions \(N\left(0, \frac{\sigma^2}{\lambda}\right)\) and \(N\left(0, \frac{\tau^2}{1-\lambda}\right)\), respectively.
	Since \(Y-X\) is a continuous function of \((X, Y)\), it follows that
	\begin{equation*}
		\sqrt{m+n}[(\bar{Y}-\bar{X})-(\eta-\xi)]\stackrel{d}{\rightarrow}N\left(0,\frac{\sigma^2}{\lambda}+\frac{\tau^2}{1-\lambda}\right),
	\end{equation*}
	or, equivalently, that
	\begin{equation*}
		\frac{(\bar{Y}-\bar{X})-(\eta-\xi)}{\sqrt{\frac{\sigma^2}{m}+\frac{\tau^2}{n}}}\stackrel{d}{\rightarrow}N(0,1).
	\end{equation*}

	More specifically, consider the probability
	\begin{equation*}
		P\{\sqrt{m+n}[(\bar{Y}-\bar{X})-(\eta-\xi)]\leq z\}.
	\end{equation*}
	By Theorem~\ref{thm:sufficient-condition-for-convergence-of-random-vectors}, since \(P(Y-X=z)=0\), it follows that
	\begin{equation*}
		P\{(Y-X) \leq z\}=\Phi\left(\frac{z}{\sqrt{\frac{\sigma^2}{\lambda}+\frac{\tau^2}{1-\lambda}}}\right).
	\end{equation*}
\end{example}

\begin{theorem}\label{thm:convergence-in-distribution-by-linear-combinations}
	A necessary and sufficient condition for
	\begin{equation*}
		(X_{1}^{(n)},\ldots,X_k^{(n)})\stackrel{d}{\rightarrow}(X_{1},\ldots,X_k),
	\end{equation*}
	is that for any constants \(c_{1},\ldots,c_k\), we have
	\begin{equation*}
		\sum_{i=1}^{k}c_{i}X_{i}^{(n)}\stackrel{d}{\rightarrow}\sum_{i=1}^{k}c_{i}X_{i}.
	\end{equation*}
\end{theorem}

\begin{example}[Orthogonal Linear Combinations]
	Let \(Y_{1},Y_{2},\ldots\) be iid with mean \(\bbE\left(Y_{i}\right)=0\) and variance \(\Var\left(Y_{i}\right)=\sigma^2\), and consider the joint distribution of the linear combinations
	\begin{equation*}
		X_{1}^{(n)}=\sum_{j=1}^{n}a_{nj}Y_{j},\quad X_{2}^{(n)}=\sum_{j=1}^{n}b_{nj}Y_{j}
	\end{equation*}
	satisfying the orthogonality conditions
	\begin{equation}
		\label{eq:orthogonality-conditions-2}
		\sum_{j=1}^{n}a_{nj}^{2}=\sum_{j=1}^{n}b_{nj}^2=1,\quad \sum_{j=1}^{n}a_{nj}b_{nj}=0.
	\end{equation}
	Then if
	\begin{equation*}
		\max_{j}a_{jn}^{2}\rightarrow 0,\quad\max_{j}b_{jn}^2\rightarrow 0,n \rightarrow \infty.
	\end{equation*}
	it follows that
	\begin{equation*}
		(X_{1}^{(n)},X_{2}^{(n)})\stackrel{d}{\rightarrow}(X_{1},X_{2}),
	\end{equation*}
	with \((X_{1},X_{2})\) independently distributed, each according to the normal distribution \(N\left(0,\sigma^2\right)\).
\end{example}

\begin{proof}
	To prove this result, it is by Theorem~\ref{thm:convergence-in-distribution-by-linear-combinations} enough to show that
	\begin{equation*}
		c_{1} X_{1}^{(n)}+c_{2} X_{2}^{(n)}=\sum\left(c_{1} a_{nj}+c_{2} b_{nj}\right) Y_{j} \stackrel{d}{\rightarrow}c_{1} X_{1}+c_{2} X_{2},
	\end{equation*}
	where the distribution of \(c_{1} X_{1}+c_{2} X_{2}\) is \(N\left(0,\left[c_{1}^2+c_{2}^2\right] \sigma^2\right)\). The sum on the left side of (5.1.24) is of the form \(\sum d_{nj} Y_{j}\) with
	\begin{equation*}
		d_{nj}=c_{1} a_{nj}+c_{2} b_{nj}
	\end{equation*}

	It follows from~\eqref{eq:orthogonality-conditions-2} that
	\begin{equation*}
		\sum_{j=1}^{n}d_{nj}^2=c_{1}^2+c_{2}^2
	\end{equation*}
	furthermore
	\begin{equation*}
		\max d_{nj}^2\leq 2\max\left[c_{1}^2a_{nj}^2+c_{2}^2 b_{nj}^2\right]\leq 2\left[c_{1}^2 \max a_{nj}^2+c_{2}^2\max b_{nj}^2\right].
	\end{equation*}
	Thus,
	\begin{equation*}
		\max_{j}d_{nj}^{2}/\sum_{j=1}^{n}d_{nj}^{2}\rightarrow 0.
	\end{equation*}

	According to Theorems~\ref{thm:linear-combination-inid}, it thus follows that
	\begin{equation*}
		\sum d_{nj} Y_{j} \stackrel{d}{\rightarrow}N\left(0,\left(c_{1}^2+c_{2}^2\right) \sigma^2\right).
	\end{equation*}
	Thus, by Theorem~\ref{thm:convergence-in-distribution-by-linear-combinations}, we have
	\begin{equation*}
		(X_{1}^{(n)},X_{2}^{(n)})\stackrel{d}{\rightarrow}(X_{1},X_{2}),
	\end{equation*}
	with \((X_{1},X_{2})\) independently distributed, each according to the normal distribution \(N\left(0,\sigma^2\right)\).
\end{proof}
