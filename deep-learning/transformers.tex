\chapter{Transformers}\label{chap:transformers}

\section{Scaled Dot-Product Attention}

The attention mechanism is a fundamental component of the Transformer architecture, enabling each token to incorporate contextual information from the entire sequence. The most widely adopted form is the \textbf{scaled dot-product attention}.

Given an input sequence \(\{\mathbf{x}_1, \dots, \mathbf{x}_n\}\) with \(\mathbf{x}_i \in \mathbb{R}^{d_{\text{model}}}\), each token is projected into three vectors:
\begin{equation*}
	\mathbf{q}_i = \mathbf{x}_i W^Q, \quad \mathbf{k}_i = \mathbf{x}_i W^K, \quad \mathbf{v}_i = \mathbf{x}_i W^V,
\end{equation*}
where \(W^Q, W^K, W^V \in \mathbb{R}^{d_{\text{model}} \times d_k}\) are learnable parameter matrices.
The attention score between the \(i\)-th and \(j\)-th tokens is defined as:
\begin{equation*}
	\text{score}_{ij} = \frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}},
\end{equation*}
where the scores are normalized using the softmax function to obtain attention weights:
\begin{equation*}
	\alpha_{ij} = \frac{\exp\left( \frac{\mathbf{q}_i^\top \mathbf{k}_j}{\sqrt{d_k}} \right)}{\sum_{l=1}^{n} \exp\left( \frac{\mathbf{q}_i^\top \mathbf{k}_l}{\sqrt{d_k}} \right)}.
\end{equation*}
The output for the \(i\)-th token is computed as a weighted sum of the value vectors:
\begin{equation*}
	\text{Attention}(\mathbf{q}_i, K, V) = \sum_{j=1}^{n} \alpha_{ij} \cdot \mathbf{v}_j.
\end{equation*}

In matrix form, let \(Q, K, V \in \mathbb{R}^{n \times d_k}\) represent the matrices of all queries, keys, and values, respectively. The attention output is given by:
\begin{equation*}
	\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V.
\end{equation*}

In tasks requiring restricted access (e.g., causal decoding or padding), a mask matrix \(M \in \mathbb{R}^{n \times n}\) can be added prior to the softmax operation:
\begin{equation*}
	\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} + M \right)V.
\end{equation*}

To enhance representational power, the Transformer employs \textbf{multi-head attention}, which allows the model to attend to information from multiple representation subspaces:
\begin{equation}
	\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O.
\end{equation}

Each attention head is computed independently:
\begin{equation}
	\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V),
\end{equation}
where \(W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_k}\) are projection matrices specific to head \(i\), and \(W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}\) is the final output projection.

This mechanism enables the model to dynamically integrate context across the sequence and has proven essential for the success of Transformer-based architectures in natural language processing and beyond.

\section{Key-Value Caching}\label{sec:kv-caching}

Transformer-based autoregressive language models, such as GPT, generate text sequentially by predicting each token conditioned on all previously generated tokens. At each generation step \(t\), the model takes the sequence \(\{x_1, x_2, \dots, x_{t-1}\}\) as input and outputs the next token \(x_t\). A computational challenge arises in this setting: without optimization, the model recomputes representations of all previous tokens at every step, leading to redundant and inefficient computations, especially for long sequences.

To address this inefficiency, \textbf{Key-Value (KV) Caching} is employed during the inference phase. This technique caches the \textit{key} and \textit{value} vectors generated by the self-attention mechanism for all previously seen tokens. Recall that in a Transformer, the self-attention operation is defined as:
\begin{equation*}
	\mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V,
\end{equation*}
where \(Q\), \(K\), and \(V\) denote the query, key, and value matrices, respectively, and \(d_k\) is the dimension of the key vectors.

During inference, the key and value vectors for past tokens are stored in the KV cache, and only the query vector for the current token needs to be computed. The attention mechanism then retrieves the necessary context by attending to the cached key-value pairs, eliminating the need to recompute past representations. Furthermore, the current token's key and value vectors are appended to the cache for use in subsequent steps.

KV caching significantly reduces the computational complexity of sequence generation and is a core optimization enabling real-time response in large language models. It is especially crucial for applications involving long-context generation, such as open-ended dialogue, document summarization, and code synthesis.
