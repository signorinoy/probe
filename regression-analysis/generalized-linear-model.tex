\chapter{Generalized Linear Model}

\section{Introduction}

Suppose the response $Y$ has a distribution in the exponential family
\begin{equation*}
	f\left(y\mid\theta,\phi\right)=\exp\left[\frac{y\theta-b(\theta)}{a(\phi)}+c(y,\phi)\right]
\end{equation*}
with link function $g$, such that,
\begin{equation}
	E\left(Y\mid\bfx\right)=\mu=g^{-1}(\eta),\quad\eta=\bfx^{\top}\bfbeta
\end{equation}
where the link function provides the relationship between the linear predictor and the mean of the distribution function. If $\eta=\theta$, the link function is called \textbf{canonical link function}.

\begin{remark}
	A generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for the response variable to have an error distribution other than the normal distribution.
\end{remark}

\begin{table}[hpt]
	\centering
	\caption{Commonly Used Link Functions}
	\begin{tabular}{clcc}
		\toprule
		Distribution & Support of Distribution  & Link Function $g(\mu)$               & Mean Function $g^{-1}(\eta)$ \\
		\midrule
		Normal       & real:$(-\infty,+\infty)$ & $\mu$                                & $\eta$                       \\
		Bernoulli    & integer: $\{0,1\}$       & $\log\left(\frac{\mu}{1-\mu}\right)$ & $\frac{1}{1+\exp(-\eta)}$    \\
		Poisson      & integer: $0,1,2,\ldots$  & $\log\left(\mu\right)$               & $\exp\left(\eta\right)$      \\
		\bottomrule
	\end{tabular}
\end{table}

\paragraph{Maximum Likelihood}

Suppose the log-likelihood function be
\begin{equation}
	\ell\left(\bfbeta\mid\bfx,y\right)=\log\left[f\left(y\mid\theta,\phi\right)\right]=\log\left[f\left(y\mid g^{-1}(\eta),\phi\right)\right]
\end{equation}
where $g$ is the canonical link function and $\eta=\bfx^{\top}\bfbeta$.

Let
\begin{equation*}
	U\left(\bfbeta\right)=\frac{\partial\ell\left(\bfbeta\right)}{\partial\bfbeta},\quad A\left(\bfbeta\right)=-\frac{\partial^{2}\ell\left(\bfbeta\right)}{\partial\bfbeta^{\prime}\partial\bfbeta}
\end{equation*}
be the score function and observed information matrix.

If $\hat{\bfbeta}$ is the maximum likelihood estimate, then
\begin{equation*}
	U\left(\hat{\bfbeta}\right)=\bfzero
\end{equation*}

By the mean value theorem,
\begin{equation*}
	\begin{aligned}
		            & U\left(\hat{\bfbeta}\right)-U\left(\bfbeta_{0}\right)=\frac{\partial U\left(\bfbeta^{*}\right)}{\partial\bfbeta}\left(\hat{\bfbeta}-\bfbeta_{0}\right) \\
		\Rightarrow & -U\left(\bfbeta_{0}\right)=-A\left(\bfbeta^{*}\right)\left(\hat{\bfbeta}-\bfbeta_{0}\right)
	\end{aligned}
\end{equation*}
where $\bfbeta^{*}\in\left[\bfbeta_{0},\hat{\bfbeta}\right]$. Thus,
\begin{equation*}
	\hat{\bfbeta}=\bfbeta_{0}+A^{-1}\left(\bfbeta^{*}\right)U\left(\bfbeta_{0}\right)
\end{equation*}

Suppose $\hat{\bfbeta}_{t},\hat{\bfbeta}_{t+1}$ be the maximum likelihood estimate at the t-th and (t+1)-th iterations, respectively. Two algorithms can be used to obtain the maximum likelihood estimate $\hat{\bfbeta}$.

\begin{enumerate}
	\item Newton-Raphson Method:
	      \begin{equation}
		      \hat{\bfbeta}_{t+1}=\hat{\bfbeta}_{t}+A^{-1}\left(\hat{\bfbeta}_{t}\right)U\left(\hat{\bfbeta}_{t}\right)\Leftrightarrow A\left(\hat{\bfbeta}_{t}\right)\hat{\bfbeta}_{t+1}=A\left(\hat{\bfbeta}_{t}\right)\hat{\bfbeta}_{t}+U\left(\hat{\bfbeta}_{t}\right)
	      \end{equation}
	      where
	      \begin{equation}
		      U\left(\bfbeta\right)=\frac{\partial\ell\left(\bfbeta\right)}{\partial\bfbeta}
	      \end{equation}
	      is the score function and
	      \begin{equation}
		      A\left(\bfbeta\right)=-\frac{\partial^{2}\ell\left(\bfbeta\right)}{\partial\bfbeta^{\prime}\partial\bfbeta}
	      \end{equation}
	      is the observed information matrix.
	\item Fisher Scoring Method:
	      \begin{equation}
		      \hat{\bfbeta}_{t+1}=\hat{\bfbeta}_{t}+I^{-1}\left(\hat{\bfbeta}_{t}\right)U\left(\hat{\bfbeta}_{t}\right)\Leftrightarrow I\left(\hat{\bfbeta}_{t}\right)\hat{\bfbeta}_{t+1}=I\left(\hat{\bfbeta}_{t}\right)\hat{\bfbeta}_{t}+U\left(\hat{\bfbeta}_{t}\right)
	      \end{equation}
	      where $U\left(\bfbeta\right)$ is the score function and
	      \begin{equation}
		      I\left(\bfbeta\right)=E\left[A\left(\bfbeta\right)\right]=-E\left[\frac{\partial^{2}\ell\left(\bfbeta\right)}{\partial\bfbeta^{\prime}\partial\bfbeta}\right]
	      \end{equation}
	      is the Fisher information matrix.
\end{enumerate}

\paragraph{Bayesian Methods}

\section{Binary Data}

Suppose
\begin{equation}
	Y\sim b\left(m,\pi\right),\quad i=1,2,\ldots,n
\end{equation}
with link function
\begin{equation}
	\eta=g\left(\pi\right)=\log\left(\frac{\pi}{1-\pi}\right)=\bfx^{\top}\bfbeta
\end{equation}
\begin{remark}

\end{remark}

The likelihood function is
\begin{equation}
	f\left(\boldsymbol{\pi}\mid\bfx,\mathbf{y}\right)=\prod_{i=1}^{n}\binom{m_{i}}{y_{i}}\pi_{i}^{y_{i}}\left(1-\pi_{i}\right)^{m_{i}-y_{i}}
\end{equation}
and the log-likelihood function is
\begin{equation}
	\begin{aligned}
		\ell\left(\bfbeta\right)= & \log\left[f\left(\boldsymbol{\pi}\mid\bfx,\mathbf{y}\right)\right]=\sum_{i=1}^{n}\ell_{i}\left(\bfbeta\right)                                                  \\
		=                         & \sum_{i=1}^{n}\left\{\log\left[\binom{m_{i}}{y_{i}}\right]+y_{i}\log\left(\pi_{i}\right)+\left(m_{i}-y_{i}\right)\log\left(1-\pi_{i}\right)\right\}            \\
		=                         & \sum_{i=1}^{n}\left[y_{i}\log\left(\frac{\pi_{i}}{1-\pi_{i}}\right)+m_{i}\log\left(1-\pi_{i}\right)\right]+\sum_{i=1}^{n}\log\left[\binom{m_{i}}{y_{i}}\right]
	\end{aligned}
\end{equation}
where
\begin{equation}
	\pi_{i}=\frac{\exp\left(\bfx_{i}^{\top}\bfbeta\right)}{1+\exp\left(\bfx_{i}^{\top}\bfbeta\right)}
\end{equation}

Thus,
\begin{gather*}
	U_{r}\left(\bfbeta\right)=\sum_{i=1}^{n}\left(y_{i}-m_{i}\pi_{i}\right)x_{ir} \\
	I_{sr}\left(\bfbeta\right)=\sum_{i=1}^{n}m_{i}\pi_{i}\left(1-\pi_{i}\right)x_{is}x_{ir}
\end{gather*}

\section{Polytomous Data}

\begin{definition}[Polytomous Data]
	A response is polytomous if the response of an individual or item in a study is \textbf{restricted to one of a fixed set of possible values}.
\end{definition}

\begin{remark}
	There are two types of scales, pure scales and compound scales\footnote{A bivariate response with one response ordinal and the other continuous is an example of compound scales.}. For pure scales, there are several types:
	\begin{enumerate}
		\item \textbf{Nominal Scale}: a scale used for labeling variables into distinct classifications and does not involve a quantitative value or order.
		\item \textbf{Ordinal Scale}: a variable measurement scale used to simply depict the order of variables and not the difference between each of the variables.
		\item \textbf{Interval Scale}: a numerical scale where the order of the variables is known as well as the difference between these variables.
	\end{enumerate}
\end{remark}

Let the category probabilities given $\bfx_{i}$ be
\begin{equation}
	\pi_{j}\left(\bfx_{i}\right)=P\left(Y=y_{j}\mid\bfx=\bfx_{i}\right)
\end{equation}
and the cumulative probabilities given $\bfx_{i}$ be
\begin{equation}
	r_{j}\left(\bfx_{i}\right)=P\left(Y\leq\sum_{r\leq j}y_{r}\mid\bfx=\bfx_{i}\right)
\end{equation}
where $i=1,2,\ldots,n,\quad j=1,2,\ldots,k$.

Here, multinomial distribution is in many ways the most natural distribution to consider in the context of a polytomous response variable. The density function of the multinomial distribution is,
\begin{equation*}
	P\left(Y_{1}=y_{1},\ldots,Y_{k}=y_{k}\right)=
	\left\{\begin{array}{ll}
		\frac{m!}{y_{1}!\cdots y_{k}!}\pi_{1}^{y_{1}}\cdot\cdots\cdot \pi_{k}^{y_{k}}, & \sum_{i=1}^{k}y_{i}=m \\
		0                                                                              & \text { otherwise }
	\end{array}\right.
\end{equation*}
for non-negative integers $y_{1},\ldots,y_{k}$.

As for the link function, we have

\paragraph*{Nominal Scale}

\begin{equation}
	\pi_{j}\left(\bfx_{i}\right)=\frac{\exp \left[\eta_{j}\left(\bfx_{i}\right)\right]}{\sum_{j=1}^{k} \exp \left[\eta_{j}\left(\bfx_{i}\right)\right]}
\end{equation}
where $\eta_{j}\left(\bfx_{i}\right)=\eta_{j}\left(\bfx_{0}\right)+\left(\bfx_{i}-\bfx_{0}\right)^{\prime}\bfbeta_{j}+\alpha_{i}$.

\paragraph*{Ordinal Scale}

\begin{enumerate}
	\item Logistic Scale:
	      \begin{equation}
		      \log\left[\frac{r_{j}\left(\bfx_{i}\right)}{1-r_{j}\left(\bfx_{i}\right)}\right]=\theta_{j}-\bfx_{i}^{\top}\bfbeta
	      \end{equation}
	\item Complementary Log-Log Scale:
	      \begin{equation}
		      \log\left\{-\log\left[1-r_{j}\left(\bfx_{i}\right)\right]\right\}=\theta_{j}-\bfx_{i}^{\top}\bfbeta
	      \end{equation}
\end{enumerate}

\paragraph*{Interval Scale}

Suppose the $j$-th category exits a cardinal number or score, $s_j$, where the difference between scores is a measure of distance between or separation of categories.

\begin{enumerate}
	\item \begin{equation}
		      \log\left[\frac{r_{j}\left(\bfx_{i}\right)}{1-r_{j}\left(\bfx_{i}\right)}\right]=\varsigma_{0}+\varsigma_{1}\left(\frac{s_{j}+s_{j+1}}{2}\right)-\bfx_{i}^{\top}\bfbeta-\bfx_{i}^{\top}\boldsymbol{\xi}\left(c_{j}-\bar{c}\right)
	      \end{equation}
	      where $c_{j}=\frac{s_{j}+s_{j+1}}{2}$ or $c_{j}=\operatorname{logit}\left(\frac{s_{j}+s_{j+1}}{2}\right)$.
	\item \begin{equation}
		      \pi_{j}\left(\bfx_{i}\right)=\frac{\exp \left[\eta_{j}\left(\bfx_{i}\right)\right]}{\sum_{j=1}^{k} \exp \left[\eta_{j}\left(\bfx_{i}\right)\right]}
	      \end{equation}
	      where $\eta_{j}\left(\bfx_{i}\right)=\eta_{j}+\left(\bfx_{i}^{\top}\bfbeta\right)s_{j}+\alpha_{i}$.
	\item \begin{equation}
		      \sum_{j=1}^{k}\pi_{j}\left(\bfx_{i}\right)s_{j}=\bfx_{i}\bfbeta
	      \end{equation}
\end{enumerate}

\section{Count Data}

Departures from the idealized Poisson model are to be expected. Therefore, we avoid the assumption of Poisson variation and assume only that
\begin{equation}
	\Var\left(Y\right)=\sigma^{2}E\left(Y\right)
\end{equation}
with link function
\begin{equation}
	\log\left(\mu\right)=\eta=\bfx^{\top}\bfbeta
\end{equation}
where $\mu=E\left(Y\mid\bfx\right)$.

For the response in the Poisson distribution, i.e.
\begin{equation*}
	P(Y=y\mid\mu)=\frac{e^{-\mu}\mu^{y}}{y!}
\end{equation*}
and the log-likelihood function is
\begin{equation}
	\ell\left(\bfbeta\right)\propto\sum_{i=1}^{n}\left(y_{i} \log\left(\mu_{i}\right)-\mu_{i}\right)
\end{equation}
where $\mu_{i}=E\left(Y\mid\bfx=\bfx_{i}\right)$.
