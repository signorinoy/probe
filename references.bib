% From file: ./generative-models/diffusion-model.bib
@inproceedings{ho2020denoising,
  title = {Denoising Diffusion Probabilistic Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
  urldate = {2023-11-20},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  langid = {english}
}

@online{dockhorn2022genie,
  title = {{{GENIE}}: Higher-Order Denoising Diffusion Solvers},
  shorttitle = {Genie},
  author = {Dockhorn, Tim and Vahdat, Arash and Kreis, Karsten},
  date = {2022-10-11},
  eprint = {2210.05475},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2210.05475},
  abstract = {Denoising diffusion models (DDMs) have emerged as a powerful class of generative models. A forward diffusion process slowly perturbs the data, while a deep model learns to gradually denoise. Synthesis amounts to solving a differential equation (DE) defined by the learnt model. Solving the DE requires slow iterative solvers for high-quality generation. In this work, we propose Higher-Order Denoising Diffusion Solvers (GENIE): Based on truncated Taylor methods, we derive a novel higher-order solver that significantly accelerates synthesis. Our solver relies on higher-order gradients of the perturbed data distribution, that is, higher-order score functions. In practice, only Jacobian-vector products (JVPs) are required and we propose to extract them from the first-order score network via automatic differentiation. We then distill the JVPs into a separate neural network that allows us to efficiently compute the necessary higher-order terms for our novel sampler during synthesis. We only need to train a small additional head on top of the first-order score network. We validate GENIE on multiple image generation benchmarks and demonstrate that GENIE outperforms all previous solvers. Unlike recent methods that fundamentally alter the generation process in DDMs, our GENIE solves the true generative DE and still enables applications such as encoding and guided sampling. Project page and code: https://nv-tlabs.github.io/GENIE.},
  langid = {english},
  pubstate = {prepublished},
  annotation = {TLDR: This work derives a novel higher-order solver that significantly accelerates synthesis in DDMs, and solves the true generative DE and still enables applications such as encoding and guided sampling.}
}

% From file: ./regression-analysis/nonparametric-regression.bib
@inproceedings{bousquet2001algorithmic,
  title     = {Algorithmic Stability and Generalization Performance},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author    = {Bousquet, Olivier and Elisseeff, André},
  date      = {2001},
  volume    = {13},
  publisher = {MIT Press},
  urldate   = {2022-01-08},
  keywords  = {Jab/Pre,linter/error}
}

@article{bousquet2002stability,
  title        = {Stability and Generalization},
  author       = {Bousquet, Olivier and Elisseeff, André},
  date         = {2002},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume       = {2},
  pages        = {499--526},
  issn         = {ISSN 1533-7928},
  url          = {https://www.jmlr.org/papers/v2/bousquet02a.html},
  urldate      = {2022-01-08},
  issue        = {Mar}
}

@article{hofmann2008kernel,
  title        = {Kernel Methods in Machine Learning},
  author       = {Hofmann, Thomas and Schölkopf, Bernhard and Smola, Alexander J.},
  date         = {2008-06-01},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  volume       = {36},
  number       = {3},
  issn         = {0090-5364},
  doi          = {10.1214/009053607000000677},
  annotation   = {TLDR: A review of machine learning methods employing positive definite kernels, ranging from binary classifiers to sophisticated methods for estimation with structured data, which include nonlinear functions as well as functions defined on nonvectorial data.}
}

@incollection{mcdiarmid1989method,
  title     = {On the Method of Bounded Differences},
  booktitle = {Surveys in {{Combinatorics}}, 1989},
  author    = {McDiarmid, Colin},
  editor    = {Siemons, J.},
  date      = {1989-08-03},
  series    = {London {{Mathematical Society Lecture Note Series}}},
  edition   = {1},
  pages     = {148--188},
  publisher = {Cambridge University Press},
  location  = {Cambridge},
  doi       = {10.1017/CBO9781107359949.008}
}


@unpublished{ofer2011algorithmic,
  title  = {Algorithmic Stability},
  author = {{Ofer Dekel} and {Thach Nguyen}},
  date   = {2011-08-03}
}


@book{scholkopf2002learning,
  title      = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
  shorttitle = {Learning with Kernels},
  author     = {Schölkopf, Bernhard},
  namea      = {Smola, Alexander J.},
  nameatype  = {collaborator},
  date       = {2002},
  series     = {Adaptive Computation and Machine Learning},
  publisher  = {MIT Press},
  location   = {Cambridge, Mass}
}



@unpublished{stephane2014lecture,
  title  = {Lecture 4: Kernels and Associated Functions},
  author = {{Stéphane Canu}},
  date   = {2014-03-04},
  url    = {https://cel.archives-ouvertes.fr/cel-01003007/file/Lecture4_Kenrels_Functions_RKHS.pdf},
  venue  = {Sao Paulo}
}
