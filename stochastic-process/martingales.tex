\chapter{Martingales}

\section{Conditional Expectation}

\begin{definition}[Conditional Expectation]

\end{definition}

\begin{example}
	\begin{enumerate}
		\item If $X\in\mcF$, then
		      \begin{equation*}
			      E\left(X\mid\mcF\right)=X.
		      \end{equation*}
		\item If $X$ is independent of $\mcF$, then
		      \begin{equation*}
			      E\left(X\mid\mcF\right)=E(X).
		      \end{equation*}
		\item If $\Omega_{1}, \Omega_{2}, \ldots$ is a finite or infinite partition of $\Omega$ into disjoint sets, each of which has a positive probability, and let $\mcF=\sigma\left(\Omega_{1},\Omega_{2},\ldots\right)$, then for each $i$,
		      \begin{equation*}
			      E\left(X\mid\mcF\right)=\frac{E\left(X;\Omega_{i}\right)}{P\left(\Omega_{i}\right)}.
		      \end{equation*}
	\end{enumerate}
\end{example}

\begin{property}

\end{property}

\section{Martingales}

Let $\mcF_{n}$ be a filtration, i.e., an increasing sequence of $\sigma$-fields.
\begin{definition}[Martingale]
	A sequence $\left\{X_{n}\right\}$ of real-valued random variables  is said to be a martingale for $\mcF_{n}$, if
	\begin{enumerate}
		\item $X_{n}$ is integrable, i.e., $E\left|X_{n}\right|<\infty$
		\item $X_{n}$ is adapted to $\mcF_{n}$, i.e., $\forall n,X_{n}\in \mcF_{n}$
		\item $X_{n}$ satisfies the martingale condition, i.e.,
		      \begin{equation}
			      E\left(X_{n+1}\mid\mcF_{n}\right)=X_{n},\quad\forall n
		      \end{equation}
	\end{enumerate}
\end{definition}

\begin{remark}
	If in the last definition, $=$ is replaced by $\leq$ or $\geq$, then $X$ is said to be a supermartingale or submartingale, respectively.
\end{remark}

\begin{example}[Linear Martingale]

\end{example}

\begin{example}[Quadratic Martingale]

\end{example}

\begin{example}[Exponential Martingale]

\end{example}

\begin{example}[Random Walk]
	Suppose $X_{n}=X_{0}+\xi_{1}+\cdots+\xi_{n}$, where $X_{0}$ is constant, $\xi_{m}$ are independent and have $E\xi_{m}=0,\sigma_{m}^{2}=E\xi_{m}^{2}<\infty$. Let $\mcF_{n}=\sigma\left(\xi_{1},\ldots,\xi_{n}\right)$ for $n\geq 1$ and take $\mcF_{0}=\{\emptyset, \Omega\}$. Show $X_{n}$ is a martingale, and $X_{n}^{2}$ is a submartingale.
\end{example}

\begin{proof}
	It is obvious that,
	\begin{equation*}
		E\left|X_{n}\right|<\infty,\quad X_{n}\in\mcF_{n}
	\end{equation*}
	Since $\xi_{n+1}$ is independent of $\mcF_{n}$, so using the linearity of conditional expectation, (4.1.1), and Example 4.1.4,
	\begin{equation*}
		E\left(X_{n+1}\mid\mcF_{n}\right)=E\left(X_{n}\mid\mcF_{n}\right)+E\left(\xi_{n+1}\mid\mcF_{n}\right)=X_{n}+E\xi_{n+1}=X_{n}
	\end{equation*}
	So $X_{n}$ is a martingale, and Theorem 4.2.6 implies $X_{n}^{2}$ is a submartingale.
\end{proof}

\begin{remark}
	If we let $\lambda=x^{2}$ and apply Theorem 4.4.2 to $X_{n}^{2}$, we get Kolmogorov's maximal inequality, Theorem 2.5.5:
	\begin{equation}
		P\left(\max_{1\leq m\leq n}\left|X_{m}\right|\geq x\right)\leq x^{-2}\operatorname{var}\left(X_{n}\right)
	\end{equation}
\end{remark}

\begin{theorem}[Orthogonality of Martingale Increments]

\end{theorem}

\begin{theorem}[Conditional Variance Formula]

\end{theorem}

\begin{definition}[Predictable Sequence]

\end{definition}

\begin{definition}[Stopping Time]

\end{definition}

\begin{theorem}[Martingale Convergence Theorem]

\end{theorem}

\section{Doob's Inequality}

\begin{example}[Doob's Decomposition]
	Given a sequence of independent random variables $\{X_k\}_{k=1}^{m}$, recall the sequence \(Y_{k}=\bbE\left[f(X)\mid X_{1},\ldots,X_{k}\right]\) previously defined, and suppose that \(\bbE\left|f(X)\right|<\infty\). We claim that \(\{Y_{k}\}_{k=1}^{n}\) is a martingale with respect to \(\{X_{k}\}_{k=1}^{n}\). Indeed, in terms of the shorthand \(X_{1}^{k}=(X_{1},\ldots,X_{k})\), we have
	\begin{equation*}
		\bbE\left|Y_{k}\right|=\bbE\left|\bbE\left[f(X)\mid X_{1}^{k}\right]\right|\leq\bbE\left|f(X)\right|<\infty
	\end{equation*}
	where the bound follows from Jensen's inequality. Tuning in to the martingale property, we have
	\begin{equation*}
		\bbE\left[Y_{k+1}\mid X_{1}^{k}\right]=\bbE\left[\bbE\left[f(X)\mid X_{1}^{k+1}\right]\mid X_{1}^{k}\right]=\bbE\left[f(X)\mid X_{1}^{k}\right]=Y_{k}
	\end{equation*}
	where the second equality follows from the tower property of conditional expectation. This establishes the martingale property of \(\{Y_{k}\}_{k=1}^{n}\) with respect to \(\{X_{k}\}_{k=1}^{n}\).
\end{example}

\begin{example}[Likelihood ratio]
	Let \(f\) and \(g\) be two mutuall absolutely continuous probability densities, and let \(\{X_{k}\}_{k=1}^{n}\) be a sequence of random variables drawn i.i.d.\ from \(f\). For each \(k>1\), let \(Y_{k}:=\prod_{l=1}^{k}\frac{g(X_{l})}{f(X_{l})}\) be the likelihood ratio based on the first \(k\) samples. Then the sequence \(\{Y_{k}\}_{k=1}^{n}\) is a martingale with respect to \(\{X_{k}\}_{k=1}^{n}\). Indeed, we have
	\begin{equation*}
		\bbE\left[Y_{k+1}\mid X_{1}^{k}\right]=\bbE\left[\frac{g(X_{k+1})}{f(X_{k+1})}\right]\prod_{l=1}^{k}\frac{g(X_{l})}{f(X_{l})}=Y_{k},
	\end{equation*}
	using the fact that \(\bbE\left[\frac{g(X_{k+1})}{f(X_{k+1})}\right]=1\).
\end{example}

A closely related notion is that of \textit{martingale difference sequences}, meaning an adapted sequence \(\{(D_{k},\mcF_{k})\}_{k=1}^{\infty}\) such that
\begin{equation*}
	\bbE\left[\left|D_{k}\right|\right]<\infty\quad\text{and}\quad\bbE\left[D_{k+1}\mid\mcF_{k}\right]=0.
\end{equation*}
As suggested by the name, the martingale difference sequence arises in a natural way from martingales, given by \(D_{k}=Y_{k}-Y_{k-1}\) for a martingale \(\{(Y_{k},\mcF_{k})\}_{k=1}^{\infty}\). 
Martingale difference sequences is a martingale.
Sum of martingale difference sequences is a martingale.

\begin{theorem}
	Let \(\{(D_{k},\mcF_{k})\}_{k=1}^{\infty}\) be a martingale difference sequence, and suppose that \(\bbE\left[\mathrm{e}^{\lambda D_{k}}\mid \mcF_{k-1}\right]\leq\mathrm{e}^{\frac{\lambda^{2}v_{k}^{2}}{2}}\) almost surely for any \(|\lambda|<1/\alpha_{k}\)/ Then the following hold:
	\begin{enumerate}
		\item The sum \(S_{n}:=\sum_{k=1}^{n}D_{k}\) is sub-exponential with parameter \(\left(\sqrt{\sum_{k=1}^{n}v_{k}^{2}},\alpha_{*}\right)\), where \(\alpha_{*}=\max_{k}\alpha_{k}\).
		\item The sum satisfies the concentration inequality
		\begin{equation*}
			\bbP\left(\left|S_{n}\right|\geq t\right)\leq\begin{cases}
				2\exp\left(-\frac{t^{2}}{2\sum_{k=1}^{n}v_{k}^{2}}\right) & \text{if }0\leq t\leq\frac{\sqrt{2\sum_{k=1}^{n}v_{k}^{2}}}{\alpha_{*}} \\
				2\exp\left(-\frac{t}{2\alpha_{*}}\right) & \text{if }t>\frac{\sqrt{2\sum_{k=1}^{n}v_{k}^{2}}}{\alpha_{*}}
			\end{cases}
		\end{equation*}
	\end{enumerate}
\end{theorem}

\begin{proof}
	We follow the standard approach of controling the moment generating function of \(S_{n}\), and then applying the Chernoff bound. FOr any scalar \(\lambda\) such that \(|\lambda|<1/\alpha_{*}\), conditioning on \(\mcF_{n-1}\), and applying the iterated expectation yields
	\begin{equation*}
		\bbE\left[\mathrm{e}^{\lambda S_{n}}\right]=\bbE\left[\bbE\left[\mathrm{e}^{\lambda S_{n-1}}\right]\bbE\left[\mathrm{e}^{\lambda D_{n}}\mid \mcF_{n-1}\right]\right]\leq \bbE\left[\mathrm{e}^{\lambda S_{n-1}}\right]\mathrm{e}^{\frac{\lambda^{2}v_{n}^{2}}{2}},
	\end{equation*}
	where the inequality follows from the stated assumption. Iterating this inequality, we obtain
	\begin{equation*}
		\bbE\left[\mathrm{e}^{\lambda S_{n}}\right]\leq\mathrm{e}^{\frac{\lambda^{2}}{2}\sum_{k=1}^{n}v_{k}^{2}},
	\end{equation*}
	valid for any \(|\lambda|<1/\alpha_{*}\). This implies that \(S_{n}\) is sub-exponential with parameter \((\sqrt{\sum_{k=1}^{n}v_{k}^{2}},\alpha_{*})\), and the Chernoff bound follows from the definition of sub-exponentiality.
\end{proof}

\begin{corollary}[Azuma-Hoeffding]
	\label{cor:azuma-hoeffding}
	Let \(\{(D_{k},\mcF_{k})\}_{k=1}^{\infty}\) be a martingale difference sequence for which there are constants \(\{(a_{k},b_{k})\}_{k=1}^{\infty}\) such that \(D_{k}\in[a_{k},b_{k}]\) almost surely for all \(k=1,2,\ldots,n\). Then for any \(t>0\), we have
	\begin{equation*}
		\bbP\left(\left|S_{n}\right|\geq t\right)\leq 2\exp\left(-\frac{t^{2}}{2\sum_{k=1}^{n}(b_{k}-a_{k})^{2}}\right).
	\end{equation*}
\end{corollary}

An important application of Corollary \ref{cor:azuma-hoeffding} concerns functions that satisfy a bounded differences property.

\begin{definition}[Bounded Differences Property]
	A function \(f:\bbR^{n}\to\bbR\) is said to satisfy the \textit{bounded differences property} with respect to a sequence of sets \(\{S_{k}\}_{k=1}^{n}\) if for any \(x_{1},\ldots,x_{n}\in\bbR^{n}\) that differ only in the \(k\)-th coordinate, we have
	\begin{equation*}
		\left|f(x_{1},\ldots,x_{k-1},x_{k},x_{k+1},\ldots,x_{n})-f(x_{1},\ldots,x_{k-1},x_{k}',x_{k+1},\ldots,x_{n})\right|\leq L_{k}
	\end{equation*}
	for any \(x,x'\in\bbR^{n}\) 
\end{definition}

For instance, if the function \(f\) is L-Lipschitz with respect to the Hamming distance, \(d_{H}(x,x')=\sum_{k=1}^{n}\mathbb{I}\{x_{k}\neq x_{k}'\}\), then  \(f\) satisfies the bounded differences property with parameters \(L\) uniformly over all \(k=1,\ldots,n\).

\begin{theorem}[Bounded Differences Inequality]
	\label{thm:bounded-differences}
	Let \(f:\bbR^{n}\to\bbR\) satisfy the bounded differences property with parameters \(\{L_{k}\}_{k=1}^{n}\) and that the random variables \(\bfX=(X_{1},\ldots,X_{n})\) has independent coordinates. Then for any \(t>0\), we have
	\begin{equation*}
		\bbP\left(\left|f(\bfX)-\bbE[f(\bfX)]\right|\geq t\right)\leq 2\exp\left(-\frac{t^{2}}{2\sum_{k=1}^{n}L_{k}^{2}}\right).
	\end{equation*}
\end{theorem}
\begin{proof}
	
\end{proof}

\begin{example}[Classical Hoeffding from Bounded Differences]
	
\end{example}

\begin{example}[U-statistics]
	
\end{example}

\begin{example}[Rademacher Complexity]
	
\end{example}

\begin{theorem}[Doob's Inequality]

\end{theorem}

\begin{theorem}[$L^{p}$ Maximum Inequality]

\end{theorem}

\section{Uniform Integrability}

\section{Optional Stopping Theorems}
